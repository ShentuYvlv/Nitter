import json
import time
import random
import asyncio
import aiohttp
import redis
import logging
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict, Set, Optional, Tuple
import os
import argparse
import sys
from pathlib import Path
import re
from dataclasses import dataclass
import hashlib

# åœ¨Windowsä¸Šè®¾ç½®æ­£ç¡®çš„äº‹ä»¶å¾ªç¯ç­–ç•¥
if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("enhanced_polling")

# Redisé…ç½® - ä»…ç”¨äºæ¨æ–‡æµ
REDIS_HOST = os.environ.get("REDIS_HOST", "localhost")
REDIS_PORT = int(os.environ.get("REDIS_PORT", 6379))
REDIS_DB = int(os.environ.get("REDIS_DB", 0))
REDIS_PASSWORD = os.environ.get("REDIS_PASSWORD", None)

# ç®€åŒ–çš„Redisé…ç½® - ä»…ç”¨äºæ¨æ–‡æµ
TWEET_STREAM_KEY = "tweets"

# çŠ¶æ€æ–‡ä»¶é…ç½®
STATE_FILE = "state.json"
DEFAULT_STATE = {
    "system": {
        "last_updated": None,
        "version": "1.0"
    },
    "users": {},
    "instances": {}
}

# è½®è¯¢é…ç½® - ä»RedisåŠ¨æ€åŠ è½½
POLLING_CONFIG_KEY = "polling_config"

# é»˜è®¤è½®è¯¢é…ç½®
DEFAULT_POLLING_CONFIG = {
    "PRIORITY_POLL_INTERVAL": 2,    # ä¼˜å…ˆç”¨æˆ·è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰
    "NORMAL_POLL_INTERVAL": 2,      # æ™®é€šç”¨æˆ·è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰
    "BATCH_SIZE": 8,                # æ¯æ‰¹å¤„ç†çš„ç”¨æˆ·æ•°
    "REQUEST_TIMEOUT": 10,           # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    "MAX_RETRIES": 5,                # æœ€å¤§é‡è¯•æ¬¡æ•°
    "REQUEST_RATE": 10.0,            # æ¯ç§’è¯·æ±‚æ•°é™åˆ¶
    "BURST_CAPACITY": 20             # çªå‘å®¹é‡
}

# å…¼å®¹æ€§é…ç½®ï¼ˆä»åŠ¨æ€é…ç½®ä¸­è·å–ï¼‰
POLL_INTERVAL = 15          # å°†ä»RedisåŠ¨æ€æ›´æ–°
CONCURRENT_USERS = 5        # å°†ä»RedisåŠ¨æ€æ›´æ–°
REQUEST_TIMEOUT = 5         # å°†ä»RedisåŠ¨æ€æ›´æ–°
MAX_RETRIES = 2             # å°†ä»RedisåŠ¨æ€æ›´æ–°
RETRY_DELAYS = [0.5, 1.0]   # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
RATE_LIMIT_DELAY = 2.0      # é‡åˆ°429é”™è¯¯æ—¶çš„é¢å¤–å»¶è¿Ÿ
BATCH_DELAY = 0.5           # æ‰¹æ¬¡é—´åŸºç¡€å»¶è¿Ÿ

@dataclass
class NitterInstance:
    url: str
    assigned_users: int = 0          # åˆ†é…çš„ç”¨æˆ·æ•°
    active_connections: int = 0      # å½“å‰æ´»è·ƒè¿æ¥æ•°
    max_connections: int = 50        # æœ€å¤§è¿æ¥æ•°
    recent_429_count: int = 0        # æœ€è¿‘çš„429é”™è¯¯è®¡æ•°
    last_reset_time: float = 0.0     # ä¸Šæ¬¡é‡ç½®æ—¶é—´
    total_requests: int = 0          # æ€»è¯·æ±‚æ•°

class StateManager:
    """çŠ¶æ€ç®¡ç†å™¨ - ä½¿ç”¨JSONæ–‡ä»¶å­˜å‚¨æ‰€æœ‰çŠ¶æ€"""
    
    def __init__(self, state_file: str = STATE_FILE):
        self.state_file = Path(state_file)
        self.state = self._load_state()
        
    def _load_state(self) -> dict:
        """åŠ è½½çŠ¶æ€æ–‡ä»¶"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    state = json.load(f)
                logger.info(f"å·²åŠ è½½çŠ¶æ€æ–‡ä»¶ï¼ŒåŒ…å« {len(state.get('users', {}))} ä¸ªç”¨æˆ·")
                return state
            except (json.JSONDecodeError, IOError) as e:
                logger.error(f"è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
                
        logger.info("åˆ›å»ºæ–°çš„çŠ¶æ€æ–‡ä»¶")
        return DEFAULT_STATE.copy()
    
    def save_state(self):
        """ä¿å­˜çŠ¶æ€åˆ°æ–‡ä»¶"""
        try:
            self.state["system"]["last_updated"] = datetime.now().isoformat()

            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            user_count = len(self.state.get("users", {}))
            logger.debug(f"å‡†å¤‡ä¿å­˜çŠ¶æ€æ–‡ä»¶: {self.state_file}, åŒ…å« {user_count} ä¸ªç”¨æˆ·")

            # æ£€æŸ¥æœ€è¿‘æ›´æ–°çš„ç”¨æˆ·
            recent_updates = []
            current_time = time.time()
            for user_id, user_data in self.state.get("users", {}).items():
                last_check = user_data.get("last_check_time", 0)
                if current_time - last_check < 300:  # 5åˆ†é’Ÿå†…æ›´æ–°çš„
                    recent_updates.append(user_id)

            if recent_updates:
                logger.debug(f"æœ€è¿‘5åˆ†é’Ÿå†…æ›´æ–°çš„ç”¨æˆ·: {recent_updates[:5]}" +
                           (f" ç­‰{len(recent_updates)}ä¸ª" if len(recent_updates) > 5 else ""))

            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, indent=2, ensure_ascii=False)

            logger.debug(f"çŠ¶æ€æ–‡ä»¶ä¿å­˜æˆåŠŸ: {self.state_file}")

        except IOError as e:
            logger.error(f"ä¿å­˜çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"ä¿å­˜çŠ¶æ€æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {e}")
    
    def get_user_state(self, user_id: str) -> dict:
        """è·å–ç”¨æˆ·çŠ¶æ€"""
        return self.state["users"].get(user_id, {})
    
    def update_user_state(self, user_id: str, **kwargs):
        """æ›´æ–°ç”¨æˆ·çŠ¶æ€"""
        if user_id not in self.state["users"]:
            self.state["users"][user_id] = {}

        # è®°å½•æ›´æ–°å‰çš„çŠ¶æ€ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        old_state = self.state["users"][user_id].copy()

        self.state["users"][user_id].update(kwargs)
        self.state["users"][user_id]["last_updated"] = datetime.now().isoformat()

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        if "last_check_time" in kwargs:
            logger.debug(f"æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€: last_check_time={kwargs['last_check_time']}")
        if "last_tweet_id" in kwargs:
            old_tweet_id = old_state.get("last_tweet_id", "æ— ")
            new_tweet_id = kwargs["last_tweet_id"]
            if old_tweet_id != new_tweet_id:
                logger.debug(f"ç”¨æˆ· {user_id} æ¨æ–‡IDæ›´æ–°: {old_tweet_id} -> {new_tweet_id}")
    
    def get_all_users(self) -> List[str]:
        """è·å–æ‰€æœ‰ç”¨æˆ·ID"""
        return list(self.state["users"].keys())
    
    def update_instance_state(self, instance_url: str, **kwargs):
        """æ›´æ–°å®ä¾‹çŠ¶æ€"""
        if instance_url not in self.state["instances"]:
            self.state["instances"][instance_url] = {"assigned_users": 0}
        
        self.state["instances"][instance_url].update(kwargs)

class EnhancedPollingEngine:
    """å¢å¼ºçš„è½®è¯¢å¼•æ“"""
    
    def __init__(self, nitter_instances: List[str], following_file: str):
        self.instances = [NitterInstance(url) for url in nitter_instances]
        self.following_file = following_file
        self.state_manager = StateManager()
        self.use_sse = True  # å¯ç”¨SSE
        self.sse_connections = {}  # å­˜å‚¨SSEè¿æ¥
        
        # å¤šå®ä¾‹è´Ÿè½½å‡è¡¡
        self.user_instance_mapping = {}  # ç”¨æˆ·åˆ°å®ä¾‹çš„æ˜ å°„
        self.instance_stats = {}         # å®ä¾‹ç»Ÿè®¡ä¿¡æ¯
        

        
        # åŠ¨æ€å¹¶å‘æ§åˆ¶ï¼ˆç°åœ¨æŒ‰å®ä¾‹ç®¡ç†ï¼‰
        self.current_concurrent = CONCURRENT_USERS
        self.recent_errors = []  # è®°å½•æœ€è¿‘çš„é”™è¯¯
        self.max_concurrent = 5  # æœ€å¤§å¹¶å‘æ•°
        self.min_concurrent = 1  # æœ€å°å¹¶å‘æ•°
        
        # å¤±è´¥ç”¨æˆ·é˜Ÿåˆ— - ç”¨äºé“¾å¼æ‰¹æ¬¡å¤„ç†
        self.pending_users = []  # éœ€è¦é‡æ–°å¤„ç†çš„ç”¨æˆ·ï¼ˆä¸»è¦æ˜¯429é”™è¯¯ï¼‰

        # åº”ç”¨å±‚ç¼“å­˜ç»Ÿè®¡
        self.cache_stats = {
            "total_requests": 0,
            "cache_hits": 0,  # å†…å®¹hashåŒ¹é…
            "cache_misses": 0,  # å†…å®¹æœ‰å˜åŒ–
            "bandwidth_saved": 0,  # ä¼°ç®—èŠ‚çœçš„å¸¦å®½
        }
        
        # è½®è¯¢ç»Ÿè®¡
        self.polling_stats = {
            "total_users": 0,
            "successful_users": 0,
            "failed_users": 0,
            "last_cycle_time": None,
            "current_cycle": 0,
            "success_rate": 0.0,
            "failed_user_list": []
        }

        # å½“å‰è½®è¯¢å‘¨æœŸçš„ç»Ÿè®¡
        self.current_cycle_stats = {
            "processed_users": set(),  # å·²å¤„ç†çš„ç”¨æˆ·
            "successful_users": set(),  # æˆåŠŸçš„ç”¨æˆ·
            "failed_users": set(),     # æœ€ç»ˆå¤±è´¥çš„ç”¨æˆ·
            "user_attempts": {}        # ç”¨æˆ·å°è¯•æ¬¡æ•°è®°å½•
        }
        
        # è¿æ¥Redis - ä»…ç”¨äºæ¨æ–‡æµ
        try:
            self.redis_client = redis.Redis(
                host=REDIS_HOST, 
                port=REDIS_PORT, 
                db=REDIS_DB,
                password=REDIS_PASSWORD,
                decode_responses=True
            )
            self.redis_client.ping()
            logger.info(f"æˆåŠŸè¿æ¥åˆ°Redis: {REDIS_HOST}:{REDIS_PORT}")
                
        except redis.ConnectionError as e:
            logger.error(f"æ— æ³•è¿æ¥åˆ°Redis: {e}")
            raise
            
        # åˆå§‹åŒ–å®ä¾‹ç»Ÿè®¡
        for instance in self.instances:
            self.instance_stats[instance.url] = {
                "requests_this_cycle": 0,
                "errors_this_cycle": 0,
                "response_times": [],
                "last_reset": time.time()
            }
            
        # åŠ è½½è½®è¯¢é…ç½®
        self.polling_config = self.load_polling_config()
        self.apply_polling_config()

        logger.info(f"åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨ {len(self.instances)} ä¸ªNitterå®ä¾‹")
        self.print_instance_info()
        
    def print_instance_info(self):
        """æ‰“å°å®ä¾‹ä¿¡æ¯"""
        logger.info(f"=== Nitterå®ä¾‹ä¿¡æ¯ ===")
        for i, instance in enumerate(self.instances):
            logger.info(f"å®ä¾‹ {i+1}: {instance.url}")
            logger.info(f"  å½“å‰è¿æ¥æ•°: {instance.active_connections}")
            logger.info(f"  æœ€å¤§è¿æ¥æ•°: {instance.max_connections}")
            logger.info(f"  åˆ†é…ç”¨æˆ·æ•°: {instance.assigned_users}")
            logger.info(f"  æœ€è¿‘429é”™è¯¯: {instance.recent_429_count}")
            logger.info(f"  æ€»è¯·æ±‚æ•°: {instance.total_requests}")
        logger.info(f"======================")
        
        # æ£€æŸ¥ç”¨æˆ·åˆ†é…æ˜ å°„
        mapping_count = len(self.user_instance_mapping)
        logger.info(f"ç”¨æˆ·å®ä¾‹æ˜ å°„æ•°é‡: {mapping_count}")
        
        # ç»Ÿè®¡æ¯ä¸ªå®ä¾‹çš„åˆ†é…æƒ…å†µ
        instance_user_count = {}
        for user_id, instance in self.user_instance_mapping.items():
            url = instance.url
            instance_user_count[url] = instance_user_count.get(url, 0) + 1
        
        logger.info("å®é™…æ˜ å°„åˆ†å¸ƒ:")
        for url, count in instance_user_count.items():
            logger.info(f"  {url}: {count} ä¸ªç”¨æˆ·")
        logger.info(f"=======================")

    def load_polling_config(self) -> dict:
        """ä»RedisåŠ è½½è½®è¯¢é…ç½®"""
        try:
            config_json = self.redis_client.get(POLLING_CONFIG_KEY)
            if config_json:
                config = json.loads(config_json)
                logger.info(f"ä»RedisåŠ è½½è½®è¯¢é…ç½®: {config}")
                return config
            else:
                logger.info("Redisä¸­æ²¡æœ‰è½®è¯¢é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                # ä¿å­˜é»˜è®¤é…ç½®åˆ°Redis
                self.redis_client.set(POLLING_CONFIG_KEY, json.dumps(DEFAULT_POLLING_CONFIG))
                return DEFAULT_POLLING_CONFIG.copy()
        except Exception as e:
            logger.error(f"åŠ è½½è½®è¯¢é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return DEFAULT_POLLING_CONFIG.copy()

    def apply_polling_config(self):
        """åº”ç”¨è½®è¯¢é…ç½®åˆ°å…¨å±€å˜é‡"""
        global POLL_INTERVAL, CONCURRENT_USERS, REQUEST_TIMEOUT, MAX_RETRIES

        # æ˜ å°„é…ç½®åˆ°å…¼å®¹çš„å…¨å±€å˜é‡
        POLL_INTERVAL = self.polling_config.get("NORMAL_POLL_INTERVAL", 60)
        CONCURRENT_USERS = min(self.polling_config.get("BATCH_SIZE", 15), 10)  # é™åˆ¶æœ€å¤§å¹¶å‘
        REQUEST_TIMEOUT = self.polling_config.get("REQUEST_TIMEOUT", 10)
        MAX_RETRIES = self.polling_config.get("MAX_RETRIES", 5)

        # æ›´æ–°å®ä¾‹çº§åˆ«çš„é…ç½®
        self.current_concurrent = CONCURRENT_USERS

        logger.info(f"åº”ç”¨è½®è¯¢é…ç½®:")
        logger.info(f"  è½®è¯¢é—´éš”: {POLL_INTERVAL}ç§’")
        logger.info(f"  å¹¶å‘ç”¨æˆ·æ•°: {CONCURRENT_USERS}")
        logger.info(f"  è¯·æ±‚è¶…æ—¶: {REQUEST_TIMEOUT}ç§’")
        logger.info(f"  æœ€å¤§é‡è¯•: {MAX_RETRIES}æ¬¡")

    def reload_polling_config(self):
        """é‡æ–°åŠ è½½è½®è¯¢é…ç½®"""
        old_config = self.polling_config.copy()
        self.polling_config = self.load_polling_config()

        if old_config != self.polling_config:
            logger.info("æ£€æµ‹åˆ°è½®è¯¢é…ç½®å˜æ›´ï¼Œé‡æ–°åº”ç”¨é…ç½®")
            self.apply_polling_config()
            return True
        return False

    def load_following_list(self) -> List[Dict]:
        """åŠ è½½å…³æ³¨ç”¨æˆ·åˆ—è¡¨"""
        try:
            with open(self.following_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æ£€æŸ¥æ•°æ®æ ¼å¼
            if isinstance(data, list):
                # æ—§æ ¼å¼ï¼šç›´æ¥æ˜¯ç”¨æˆ·åˆ—è¡¨
                following_list = data
            elif isinstance(data, dict) and ('priority_users' in data or 'normal_users' in data):
                # æ–°æ ¼å¼ï¼šåˆ†ç»„æ ¼å¼
                following_list = []

                # æ·»åŠ ä¼˜å…ˆç”¨æˆ·
                priority_users = data.get('priority_users', [])
                for user in priority_users:
                    following_list.append({
                        'userId': user,
                        'username': user,
                        'priority': True
                    })

                # æ·»åŠ æ™®é€šç”¨æˆ·
                normal_users = data.get('normal_users', [])
                for user in normal_users:
                    following_list.append({
                        'userId': user,
                        'username': user,
                        'priority': False
                    })

                logger.info(f"åŠ è½½åˆ†ç»„æ ¼å¼é…ç½®: {len(priority_users)} ä¸ªä¼˜å…ˆç”¨æˆ·, {len(normal_users)} ä¸ªæ™®é€šç”¨æˆ·")
            else:
                logger.error(f"ä¸æ”¯æŒçš„é…ç½®æ–‡ä»¶æ ¼å¼: {type(data)}")
                return []

            logger.info(f"æˆåŠŸåŠ è½½ {len(following_list)} ä¸ªå…³æ³¨ç”¨æˆ·")
            return following_list

        except (json.JSONDecodeError, FileNotFoundError) as e:
            logger.error(f"æ— æ³•åŠ è½½å…³æ³¨åˆ—è¡¨: {e}")
            return []
            
    def get_instance_for_user(self, user_id: str) -> NitterInstance:
        """ä¸ºç”¨æˆ·è·å–å®ä¾‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œè½®è¯¢åˆ†é…ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰åˆ†é…
        if user_id in self.user_instance_mapping:
            assigned_instance = self.user_instance_mapping[user_id]
            logger.debug(f"ç”¨æˆ· {user_id} ä½¿ç”¨å·²åˆ†é…å®ä¾‹ {assigned_instance.url}")
            return assigned_instance
        
        # ç®€å•è½®è¯¢åˆ†é…ï¼šé€‰æ‹©åˆ†é…ç”¨æˆ·æœ€å°‘çš„å®ä¾‹
        selected_instance = min(self.instances, key=lambda x: x.assigned_users)
        
        # æ›´æ–°æ˜ å°„å’Œè®¡æ•°
        self.user_instance_mapping[user_id] = selected_instance
        selected_instance.assigned_users += 1
        
        logger.debug(f"ä¸ºç”¨æˆ· {user_id} åˆ†é…å®ä¾‹ {selected_instance.url}")
        return selected_instance
    
    def update_instance_stats(self, instance: NitterInstance, success: bool, duration: float, is_429: bool = False):
        """æ›´æ–°å®ä¾‹ç»Ÿè®¡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        instance.total_requests += 1

        if is_429:
            instance.recent_429_count += 1

        # å®šæœŸé‡ç½®429è®¡æ•°ï¼ˆæ¯å°æ—¶ï¼‰
        current_time = time.time()
        if current_time - instance.last_reset_time > 3600:  # 1å°æ—¶
            instance.recent_429_count = 0
            instance.last_reset_time = current_time
            logger.debug(f"é‡ç½®å®ä¾‹ {instance.url} 429è®¡æ•°")
    
    def rebalance_users(self):
        """é‡æ–°å¹³è¡¡ç”¨æˆ·åˆ†é…ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        total_users = len(self.user_instance_mapping)
        instance_count = len(self.instances)

        if instance_count == 0:
            logger.error("æ²¡æœ‰å¯ç”¨å®ä¾‹ï¼Œæ— æ³•é‡æ–°å¹³è¡¡")
            return

        users_per_instance = total_users // instance_count
        extra_users = total_users % instance_count

        logger.info(f"å¼€å§‹é‡æ–°å¹³è¡¡ {total_users} ä¸ªç”¨æˆ·åˆ° {instance_count} ä¸ªå®ä¾‹")

        # é‡ç½®æ‰€æœ‰å®ä¾‹çš„ç”¨æˆ·è®¡æ•°
        for instance in self.instances:
            instance.assigned_users = 0

        # è·å–æ‰€æœ‰ç”¨æˆ·åˆ—è¡¨
        all_users = list(self.user_instance_mapping.keys())
        user_index = 0

        for i, instance in enumerate(self.instances):
            # è®¡ç®—è¿™ä¸ªå®ä¾‹åº”è¯¥åˆ†é…å¤šå°‘ç”¨æˆ·
            target_users = users_per_instance + (1 if i < extra_users else 0)

            for _ in range(target_users):
                if user_index < len(all_users):
                    user_id = all_users[user_index]
                    self.user_instance_mapping[user_id] = instance
                    instance.assigned_users += 1
                    user_index += 1

            logger.info(f"å®ä¾‹ {instance.url} é‡æ–°åˆ†é…äº† {instance.assigned_users} ä¸ªç”¨æˆ·")

    def reset_cycle_stats(self):
        """é‡ç½®å½“å‰è½®è¯¢å‘¨æœŸç»Ÿè®¡"""
        self.current_cycle_stats = {
            "processed_users": set(),
            "successful_users": set(),
            "failed_users": set(),
            "user_attempts": {}
        }

    def update_batch_stats(self, batch_results: List[Tuple[str, bool, str]]):
        """æ›´æ–°æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯ï¼ˆç´¯è®¡åˆ°å½“å‰è½®è¯¢å‘¨æœŸï¼‰å¹¶å®æ—¶æ›´æ–°æ€»ä½“ç»Ÿè®¡"""
        for user_id, success, error_msg in batch_results:
            # è®°å½•ç”¨æˆ·å·²è¢«å¤„ç†
            self.current_cycle_stats["processed_users"].add(user_id)

            # è®°å½•å°è¯•æ¬¡æ•°
            if user_id not in self.current_cycle_stats["user_attempts"]:
                self.current_cycle_stats["user_attempts"][user_id] = []
            self.current_cycle_stats["user_attempts"][user_id].append((success, error_msg))

            # å¦‚æœæˆåŠŸï¼Œä»å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if success:
                self.current_cycle_stats["successful_users"].add(user_id)
                self.current_cycle_stats["failed_users"].discard(user_id)
            else:
                # 429é”™è¯¯ä¸ç®—å¤±è´¥ï¼Œä¼šç»§ç»­é‡è¯•
                # åªæœ‰çœŸæ­£çš„é”™è¯¯ï¼ˆç½‘ç»œã€è§£æç­‰ï¼‰æ‰ç®—å¤±è´¥
                if "429" not in error_msg and "Rate Limited" not in error_msg:
                    self.current_cycle_stats["failed_users"].add(user_id)
                    self.current_cycle_stats["successful_users"].discard(user_id)
                # 429é”™è¯¯çš„ç”¨æˆ·ä¿æŒåœ¨å¤„ç†ä¸­çŠ¶æ€ï¼Œä¸ç®—æˆåŠŸä¹Ÿä¸ç®—å¤±è´¥

        # å®æ—¶æ›´æ–°æ€»ä½“ç»Ÿè®¡ï¼ˆæ¯æ‰¹æ¬¡éƒ½æ›´æ–°ï¼‰
        self.update_realtime_stats()

    def update_realtime_stats(self):
        """å®æ—¶æ›´æ–°æ€»ä½“ç»Ÿè®¡ï¼ˆæ¯æ‰¹æ¬¡è°ƒç”¨ï¼‰"""
        successful_users = self.current_cycle_stats["successful_users"]
        failed_users = self.current_cycle_stats["failed_users"]
        processed_users = self.current_cycle_stats["processed_users"]

        # è®¡ç®—æ€»ç”¨æˆ·æ•°ï¼ˆåº”è¯¥æ˜¯æ‰€æœ‰éœ€è¦å¤„ç†çš„ç”¨æˆ·ï¼‰
        all_users = self.state_manager.get_all_users()
        total_users = len(all_users)

        # è®¡ç®—å¤„ç†ä¸­çš„ç”¨æˆ·ï¼ˆpendingé˜Ÿåˆ—ä¸­çš„ç”¨æˆ·ï¼Œä¸»è¦æ˜¯429é”™è¯¯ï¼‰
        pending_count = len(self.pending_users)

        # æ›´æ–°å®æ—¶ç»Ÿè®¡æ•°æ®
        self.polling_stats["successful_users"] = len(successful_users)
        self.polling_stats["failed_users"] = len(failed_users)
        self.polling_stats["total_users"] = total_users
        self.polling_stats["processed_users"] = len(processed_users)
        self.polling_stats["pending_users"] = pending_count
        self.polling_stats["failed_user_list"] = list(failed_users)
        self.polling_stats["last_cycle_time"] = datetime.now().isoformat()

        if total_users > 0:
            self.polling_stats["success_rate"] = len(successful_users) / total_users
        else:
            self.polling_stats["success_rate"] = 0.0

        # ä¿å­˜åˆ°Redisï¼ˆå®æ—¶æ›´æ–°ï¼‰
        try:
            self.redis_client.set("polling_stats", json.dumps(self.polling_stats))

            # æ›´è¯¦ç»†çš„æ—¥å¿—
            processing_users = len(processed_users) - len(successful_users) - len(failed_users)
            logger.info(f"ğŸ“Š å®æ—¶ç»Ÿè®¡: {len(successful_users)}/{total_users} æˆåŠŸ, "
                       f"{len(failed_users)} å¤±è´¥, {pending_count} å¾…é‡è¯• - "
                       f"æˆåŠŸç‡: {self.polling_stats['success_rate']:.1%}")

        except Exception as e:
            logger.error(f"ä¿å­˜å®æ—¶ç»Ÿè®¡å¤±è´¥: {e}")

    def finalize_cycle_stats(self):
        """å®Œæˆå½“å‰è½®è¯¢å‘¨æœŸï¼Œè¾“å‡ºæœ€ç»ˆç»Ÿè®¡æ‘˜è¦"""
        successful_users = self.current_cycle_stats["successful_users"]
        failed_users = self.current_cycle_stats["failed_users"]
        total_users = len(self.current_cycle_stats["processed_users"])

        # æœ€åä¸€æ¬¡æ›´æ–°ç»Ÿè®¡ï¼ˆç¡®ä¿æ•°æ®æœ€æ–°ï¼‰
        self.update_realtime_stats()

        # è¾“å‡ºå‘¨æœŸå®Œæˆæ‘˜è¦
        logger.info(f"ğŸ è½®è¯¢å‘¨æœŸå®Œæˆ: {len(successful_users)}/{total_users} æˆåŠŸ "
                   f"({len(failed_users)} æœ€ç»ˆå¤±è´¥) - æˆåŠŸç‡: {self.polling_stats['success_rate']:.1%}")

        if failed_users:
            logger.warning(f"æœ€ç»ˆå¤±è´¥ç”¨æˆ·: {', '.join(list(failed_users)[:5])}" +
                         (f" ç­‰{len(failed_users)}ä¸ª" if len(failed_users) > 5 else ""))

        # è¯¦ç»†é‡è¯•ç»Ÿè®¡
        retry_stats = {}
        for user_id, attempts in self.current_cycle_stats["user_attempts"].items():
            retry_count = len(attempts) - 1  # å‡å»åˆæ¬¡å°è¯•
            if retry_count > 0:
                retry_stats[user_id] = retry_count

        if retry_stats:
            logger.info(f"é‡è¯•ç»Ÿè®¡: {len(retry_stats)} ä¸ªç”¨æˆ·éœ€è¦é‡è¯•ï¼Œ"
                       f"å¹³å‡é‡è¯• {sum(retry_stats.values()) / len(retry_stats):.1f} æ¬¡")
    
    def print_load_distribution(self):
        """æ‰“å°è´Ÿè½½åˆ†å¸ƒæƒ…å†µ"""
        logger.info("=== å®ä¾‹è´Ÿè½½åˆ†å¸ƒ ===")
        for instance in self.instances:
            logger.info(f"ğŸ”µ {instance.url}:")
            logger.info(f"  åˆ†é…ç”¨æˆ·: {instance.assigned_users}")
            logger.info(f"  æ´»è·ƒè¿æ¥: {instance.active_connections}")
            logger.info(f"  æœ€è¿‘429é”™è¯¯: {instance.recent_429_count}")
        logger.info("==================")
    
    async def setup_sse_connection(self, user_id: str):
        """ä¸ºç”¨æˆ·å»ºç«‹SSEè¿æ¥"""
        instance = self.get_instance_for_user(user_id)
        sse_url = f"{instance.url}/stream/user/{user_id}"
        
        try:
            session = aiohttp.ClientSession()
            response = await session.get(
                sse_url,
                headers={"Accept": "text/event-stream"},
                timeout=aiohttp.ClientTimeout(total=None)  # SSEéœ€è¦æ— é™è¶…æ—¶
            )
            
            if response.status == 200:
                self.sse_connections[user_id] = {
                    "session": session,
                    "response": response,
                    "instance": instance
                }
                instance.active_connections += 1
                logger.info(f"ä¸ºç”¨æˆ· {user_id} å»ºç«‹SSEè¿æ¥: {sse_url}")
                
                # å¯åŠ¨SSEæ•°æ®å¤„ç†
                asyncio.create_task(self.process_sse_stream(user_id))
                return True
            else:
                await session.close()
                logger.warning(f"SSEè¿æ¥å¤±è´¥: {user_id}, çŠ¶æ€ç : {response.status}")
                return False
                
        except Exception as e:
            logger.error(f"å»ºç«‹SSEè¿æ¥æ—¶å‡ºé”™: {user_id}, {e}")
            return False
    
    async def process_sse_stream(self, user_id: str):
        """å¤„ç†SSEæ•°æ®æµ"""
        connection_info = self.sse_connections.get(user_id)
        if not connection_info:
            return
            
        response = connection_info["response"]
        session = connection_info["session"]
        
        try:
            async for line in response.content:
                line = line.decode('utf-8').strip()
                
                if line.startswith('data: '):
                    data = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€
                    try:
                        tweet_data = json.loads(data)
                        # å¤„ç†æ¨æ–‡æ•°æ®...
                        await self.process_tweet_from_sse(user_id, tweet_data)
                    except json.JSONDecodeError:
                        continue
                        
        except Exception as e:
            logger.error(f"å¤„ç†SSEæµæ—¶å‡ºé”™: {user_id}, {e}")
        finally:
            # æ¸…ç†è¿æ¥
            if user_id in self.sse_connections:
                del self.sse_connections[user_id]
                connection_info["instance"].active_connections -= 1
            await session.close()
    
    async def fetch_user_rss(self, session: aiohttp.ClientSession, user_id: str) -> bool:
        """è·å–ç”¨æˆ·RSSå†…å®¹ï¼ˆå¸¦åº”ç”¨å±‚ç¼“å­˜ä¼˜åŒ–ï¼‰"""
        instance = self.get_instance_for_user(user_id)
        url = f"{instance.url}/{user_id}/rss"

        user_state = self.state_manager.get_user_state(user_id)
        start_time = time.time()
        success = False
        is_429 = False
        
        try:
            async with session.get(url, timeout=self.polling_config.get("REQUEST_TIMEOUT", 10)) as response:
                request_duration = time.time() - start_time
                
                if response.status == 200:
                    content_text = await response.text()
                    content_size = len(content_text)
                    self.cache_stats["total_requests"] += 1
                    
                    # è®¡ç®—å†…å®¹hash
                    import hashlib
                    content_hash = hashlib.md5(content_text.encode('utf-8')).hexdigest()
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„hash
                    cached_hash = user_state.get("content_hash")
                    
                    if cached_hash == content_hash:
                        # ç¼“å­˜å‘½ä¸­ï¼å†…å®¹æ²¡æœ‰å˜åŒ–
                        self.cache_stats["cache_hits"] += 1
                        self.cache_stats["bandwidth_saved"] += content_size
                        
                        logger.info(f"ğŸ¯ ç”¨æˆ· {user_id} å†…å®¹ç¼“å­˜å‘½ä¸­ï¼è€—æ—¶: {request_duration:.2f}ç§’ï¼Œå¤§å°: {content_size} å­—èŠ‚ [å®ä¾‹: {instance.url}]")
                        
                        # æ›´æ–°æ£€æŸ¥æ—¶é—´ä½†ä¸å¤„ç†å†…å®¹
                        self.state_manager.update_user_state(
                            user_id,
                            last_check_time=time.time(),
                            last_success_time=time.time()
                        )
                        logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šå†…å®¹ç¼“å­˜å‘½ä¸­")
                        
                        success = True
                        result = False  # æ²¡æœ‰æ–°å†…å®¹
                    else:
                        # ç¼“å­˜å¤±æ•ˆï¼Œå†…å®¹æœ‰å˜åŒ–
                        self.cache_stats["cache_misses"] += 1
                        
                        logger.info(f"ğŸ“¥ ç”¨æˆ· {user_id} å†…å®¹å·²æ›´æ–°ï¼Œè€—æ—¶: {request_duration:.2f}ç§’ï¼Œå¤§å°: {content_size} å­—èŠ‚ [å®ä¾‹: {instance.url}]")
                        
                        # ä¿å­˜æ–°çš„å†…å®¹hash
                        self.state_manager.update_user_state(
                            user_id,
                            content_hash=content_hash,
                            last_check_time=time.time()
                        )
                        
                        # å¤„ç†RSSå†…å®¹
                        result = await self.process_rss_content(user_id, content_text)
                        success = True
                    
                elif response.status == 429:
                    is_429 = True
                    # 429é”™è¯¯ä¹Ÿè¦æ›´æ–°æ£€æŸ¥æ—¶é—´
                    self.state_manager.update_user_state(
                        user_id,
                        last_check_time=time.time(),
                        rate_limit_count=user_state.get("rate_limit_count", 0) + 1
                    )
                    logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼š429é™æµé”™è¯¯")
                    
                    # æŠ›å‡ºç‰¹æ®Šå¼‚å¸¸ä»¥ä¾¿åœ¨æ‰¹æ¬¡å¤„ç†ä¸­è¯†åˆ«
                    raise aiohttp.ClientResponseError(
                        request_info=response.request_info,
                        history=response.history,
                        status=429,
                        message="Rate Limited"
                    )
                else:
                    logger.warning(f"âŒ ç”¨æˆ· {user_id} è·å–å¤±è´¥: HTTP {response.status} [å®ä¾‹: {instance.url}]")
                    # å…¶ä»–HTTPé”™è¯¯ä¹Ÿè¦æ›´æ–°æ£€æŸ¥æ—¶é—´
                    self.state_manager.update_user_state(
                        user_id,
                        last_check_time=time.time(),
                        http_error_count=user_state.get("http_error_count", 0) + 1
                    )
                    logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šHTTP {response.status} é”™è¯¯")
                    success = False
                    result = False
                    
        except aiohttp.ClientResponseError as e:
            request_duration = time.time() - start_time
            if e.status == 429:
                is_429 = True
                # åªæ‰“å°ä¸€æ¡429é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…å«å®Œæ•´ä¿¡æ¯
                logger.warning(f"â° ç”¨æˆ· {user_id} é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œè€—æ—¶: {request_duration:.2f}ç§’ [å®ä¾‹: {instance.url}]")
            else:
                logger.error(f"ğŸ’¥ ç”¨æˆ· {user_id} HTTPé”™è¯¯: {e.status}ï¼Œè€—æ—¶: {request_duration:.2f}ç§’ [å®ä¾‹: {instance.url}]")
            success = False
            result = False
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿ä¸Šå±‚å¤„ç†
            
        except Exception as e:
            request_duration = time.time() - start_time
            logger.error(f"ğŸ’¥ ç”¨æˆ· {user_id} ç½‘ç»œé”™è¯¯: {e}ï¼Œè€—æ—¶: {request_duration:.2f}ç§’ [å®ä¾‹: {instance.url}]")
            # ç½‘ç»œé”™è¯¯ä¹Ÿè¦æ›´æ–°æ£€æŸ¥æ—¶é—´
            self.state_manager.update_user_state(
                user_id,
                last_check_time=time.time(),
                network_error_count=user_state.get("network_error_count", 0) + 1
            )
            logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šç½‘ç»œé”™è¯¯")
            success = False
            result = False
            
        finally:
            # æ›´æ–°å®ä¾‹ç»Ÿè®¡
            request_duration = time.time() - start_time
            self.update_instance_stats(instance, success, request_duration, is_429)
            
        return result if success else False
    
    def print_cache_stats(self):
        """æ‰“å°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.cache_stats["total_requests"]

        if total_requests > 0:
            hit_rate = (self.cache_stats["cache_hits"] / total_requests) * 100
            saved_mb = self.cache_stats["bandwidth_saved"] / (1024 * 1024)

            logger.info(f"""
ğŸ“Š åº”ç”¨å±‚ç¼“å­˜ç»Ÿè®¡:
   æ€»è¯·æ±‚æ•°: {total_requests}
   ç¼“å­˜å‘½ä¸­: {self.cache_stats["cache_hits"]} ({hit_rate:.1f}%)
   ç¼“å­˜å¤±è¯¯: {self.cache_stats["cache_misses"]}
   èŠ‚çœå¸¦å®½: {saved_mb:.1f} MB
            """)
        else:
            logger.info("ğŸ“Š åº”ç”¨å±‚ç¼“å­˜ç»Ÿè®¡: æš‚æ— æ•°æ®")
    
    async def process_rss_content(self, user_id: str, content: str) -> bool:
        """å¤„ç†RSSå†…å®¹"""
        tweet_data = None  # é¢„å®šä¹‰å˜é‡é¿å…ä½œç”¨åŸŸé—®é¢˜
        
        try:
            root = ET.fromstring(content)
            items = root.findall(".//item")
            
            if not items:
                logger.debug(f"ç”¨æˆ· {user_id} RSSæ²¡æœ‰æ¨æ–‡é¡¹ç›®")
                return False
                
            # å°è¯•ä»RSSä¸­æå–ç”¨æˆ·å
            username = user_id  # é»˜è®¤ä½¿ç”¨user_id
            
            # æ–¹æ³•1: ä»channel titleä¸­æå–ç”¨æˆ·å (æ ¼å¼é€šå¸¸æ˜¯ "/ Twitter")
            channel_title = root.find(".//channel/title")
            if channel_title is not None and channel_title.text:
                title_text = channel_title.text
                # å¤„ç†ä¸åŒçš„æ ‡é¢˜æ ¼å¼
                if "/ Twitter" in title_text:
                    username = title_text.replace("/ Twitter", "").strip()
                elif "/ Nitter" in title_text:
                    username = title_text.replace("/ Nitter", "").strip()
                elif " / @" in title_text:
                    # å¤„ç† 'Aster / @Aster_DEX' è¿™ç§æ ¼å¼ï¼Œåªå–"/"å‰é¢çš„éƒ¨åˆ†
                    username = title_text.split(" / @")[0].strip()
                elif " /" in title_text:
                    # å¤„ç†å…¶ä»–åŒ…å«"/"çš„æ ¼å¼ï¼Œå–"/"å‰é¢çš„éƒ¨åˆ†
                    username = title_text.split(" /")[0].strip()
                else:
                    # å¦‚æœæ²¡æœ‰ç‰¹æ®Šæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨æ ‡é¢˜
                    username = title_text.strip()
                
                logger.debug(f"ä»RSS channel titleæå–ç”¨æˆ·å: {user_id} -> {username} (åŸæ ‡é¢˜: {title_text})")
                
            # æ–¹æ³•2: ä»ç¬¬ä¸€ä¸ªæ¨æ–‡çš„ä½œè€…ä¿¡æ¯ä¸­æå– (å¤‡é€‰æ–¹æ¡ˆ)
            if username == user_id:
                latest_item = items[0]
                # å°è¯•ä»creatoræˆ–authorå­—æ®µè·å–
                creator = latest_item.find(".//{http://purl.org/dc/elements/1.1/}creator")
                if creator is not None and creator.text:
                    username = creator.text.strip()
                    logger.debug(f"ä»RSS creatorå­—æ®µæå–ç”¨æˆ·å: {user_id} -> {username}")
                
                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•ä»æ¨æ–‡æ ‡é¢˜ä¸­æå– (é€šå¸¸æ ¼å¼æ˜¯ "RT by username: content")
                if username == user_id:
                    title = latest_item.find("title")
                    if title is not None and title.text:
                        title_text = title.text
                        if "RT by " in title_text and ":" in title_text:
                            # æå– "RT by username:" ä¸­çš„ç”¨æˆ·å
                            parts = title_text.split("RT by ")[1].split(":")[0]
                            username = parts.strip()
                            logger.debug(f"ä»æ¨æ–‡æ ‡é¢˜æå–ç”¨æˆ·å: {user_id} -> {username}")
                
            # è·å–æœ€æ–°æ¨æ–‡
            latest_item = items[0]
            link = latest_item.find("link")
            title = latest_item.find("title")
            description = latest_item.find("description")
            pub_date = latest_item.find("pubDate")
            
            # æå–å›¾ç‰‡URL
            images = []

            # æ–¹æ³•1: ä»enclosureå…ƒç´ ä¸­æå–å›¾ç‰‡
            enclosures = latest_item.findall("enclosure")
            for enclosure in enclosures:
                if enclosure.get("type", "").startswith("image/"):
                    url = enclosure.get("url", "")
                    if url:
                        images.append(url)

            # æ–¹æ³•2: ä»descriptionçš„HTMLå†…å®¹ä¸­æå–å›¾ç‰‡
            if description is not None and description.text:
                # åŒ¹é…imgæ ‡ç­¾ä¸­çš„srcå±æ€§
                img_pattern = r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>'
                img_matches = re.findall(img_pattern, description.text, re.IGNORECASE)
                images.extend(img_matches)

                # æ³¨æ„ï¼šä¸å†ä½¿ç”¨é€šç”¨URLæ¨¡å¼åŒ¹é…ï¼Œå› ä¸ºå®ƒä¼šé‡å¤æå–å·²ç»ä»imgæ ‡ç­¾æå–çš„å›¾ç‰‡
                # åªæœ‰åœ¨æ²¡æœ‰ä»imgæ ‡ç­¾æ‰¾åˆ°å›¾ç‰‡æ—¶ï¼Œæ‰ä½¿ç”¨URLæ¨¡å¼åŒ¹é…ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
                if not img_matches:
                    url_pattern = r'https?://[^\s<>"]+\.(?:jpg|jpeg|png|gif|webp)(?:\?[^\s<>"]*)?'
                    url_matches = re.findall(url_pattern, description.text, re.IGNORECASE)
                    images.extend(url_matches)

            # æ ‡å‡†åŒ–å’Œå»é‡å›¾ç‰‡URL
            def normalize_image_url(url):
                """æ ‡å‡†åŒ–å›¾ç‰‡URLï¼Œç”¨äºå»é‡"""
                if not url:
                    return ""

                # ä¿®å¤ç«¯å£å·é—®é¢˜ï¼šå¦‚æœURLæ˜¯localhostä½†æ²¡æœ‰ç«¯å£ï¼Œæ·»åŠ 8080ç«¯å£
                if url.startswith('http://localhost/') and ':8080' not in url:
                    url = url.replace('http://localhost/', 'http://localhost:8080/')

                # ç§»é™¤URLä¸­çš„æŸ¥è¯¢å‚æ•°è¿›è¡Œå»é‡æ¯”è¾ƒï¼ˆä½†ä¿ç•™åŸå§‹URLï¼‰
                from urllib.parse import urlparse
                parsed = urlparse(url)
                normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                return normalized

            # å»é‡å¹¶è¿‡æ»¤æœ‰æ•ˆçš„å›¾ç‰‡URL
            unique_images = []
            seen_normalized = set()

            for img_url in images:
                if not img_url or img_url.startswith('data:') or len(img_url) <= 10:
                    continue

                # ä¿®å¤ç«¯å£å·é—®é¢˜
                if img_url.startswith('http://localhost/') and ':8080' not in img_url:
                    img_url = img_url.replace('http://localhost/', 'http://localhost:8080/')
                    logger.debug(f"ä¿®å¤å›¾ç‰‡URLç«¯å£: {img_url}")

                # ä½¿ç”¨æ ‡å‡†åŒ–URLè¿›è¡Œå»é‡
                normalized = normalize_image_url(img_url)
                if normalized not in seen_normalized:
                    seen_normalized.add(normalized)
                    unique_images.append(img_url)
            
            if unique_images:
                logger.debug(f"ç”¨æˆ· {user_id} æ¨æ–‡åŒ…å« {len(unique_images)} å¼ å›¾ç‰‡: {unique_images[:2]}...")  # åªæ˜¾ç¤ºå‰2ä¸ªURL
                # æ·»åŠ è¯¦ç»†çš„å›¾ç‰‡æå–è°ƒè¯•ä¿¡æ¯
                logger.debug(f"åŸå§‹å›¾ç‰‡åˆ—è¡¨é•¿åº¦: {len(images)}, å»é‡å: {len(unique_images)}")
                if len(images) != len(unique_images):
                    logger.debug(f"æ£€æµ‹åˆ°é‡å¤å›¾ç‰‡ï¼ŒåŸå§‹: {images}, å»é‡å: {unique_images}")
            else:
                logger.debug(f"ç”¨æˆ· {user_id} æ¨æ–‡ä¸åŒ…å«å›¾ç‰‡")
            
            # ä¿®å¤æ£€æŸ¥é€»è¾‘ - æ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ–‡æœ¬å†…å®¹
            if link is None or not link.text:
                logger.warning(f"ç”¨æˆ· {user_id} æ¨æ–‡é“¾æ¥ä¸ºç©º")
                return False
            
            if title is None or not title.text:
                logger.warning(f"ç”¨æˆ· {user_id} æ¨æ–‡æ ‡é¢˜ä¸ºç©º")
                return False
                
            if description is None or not description.text:
                logger.warning(f"ç”¨æˆ· {user_id} æ¨æ–‡æè¿°ä¸ºç©º")
                return False
                
            if pub_date is None or not pub_date.text:
                logger.warning(f"ç”¨æˆ· {user_id} æ¨æ–‡å‘å¸ƒæ—¶é—´ä¸ºç©º")
                return False
            
            tweet_id = link.text.split("/")[-1]
            user_state = self.state_manager.get_user_state(user_id)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°æ¨æ–‡
            current_last_tweet_id = user_state.get("last_tweet_id")
            logger.debug(f"ç”¨æˆ· {user_id} å½“å‰ä¿å­˜çš„tweet_id: {current_last_tweet_id}, æ–°tweet_id: {tweet_id}")
            
            if current_last_tweet_id == tweet_id:
                logger.debug(f"ç”¨æˆ· {user_id} æ¨æ–‡æœªæ›´æ–°ï¼Œè·³è¿‡")
                # å³ä½¿æ²¡æœ‰æ–°æ¨æ–‡ï¼Œä¹Ÿè¦æ›´æ–°æœ€åæ£€æŸ¥æ—¶é—´å’ŒæˆåŠŸæ—¶é—´
                self.state_manager.update_user_state(
                    user_id,
                    last_check_time=time.time(),
                    last_success_time=time.time(),
                    username=username
                )
                logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šæ— æ–°æ¨æ–‡ä½†æ›´æ–°æ£€æŸ¥æ—¶é—´")
                return False
                        
            # è§£æå‘å¸ƒæ—¶é—´
            try:
                pub_time = self.parse_date(pub_date.text)
                today = datetime.now().date()
                
                # å¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œï¼Œåªæ¨é€å½“æ—¥æ¨æ–‡
                if not user_state.get("initialized") and pub_time.date() != today:
                    logger.info(f"ç”¨æˆ· {user_id} é¦–æ¬¡è¿è¡Œï¼Œè·³è¿‡éå½“æ—¥æ¨æ–‡: {pub_time.date()}")
                    self.state_manager.update_user_state(
                        user_id, 
                        last_tweet_id=tweet_id, 
                        initialized=True,
                        last_check_time=time.time(),
                        username=username
                    )
                    logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šé¦–æ¬¡è¿è¡Œåˆå§‹åŒ–")
                    return False
                    
            except Exception as e:
                logger.warning(f"è§£æç”¨æˆ· {user_id} æ¨æ–‡æ—¶é—´å¤±è´¥: {e}")
                pub_time = datetime.now()
            
            # æ„å»ºæ¨æ–‡æ•°æ®


            tweet_data = {
                "id": tweet_id,
                "user_id": user_id,
                "username": username,
                "content": title.text or "",
                "html": description.text or "",
                "published_at": pub_date.text,
                "url": link.text,
                "timestamp": pub_time.isoformat(),
                "images": json.dumps(unique_images)
            }

            # æ·»åŠ åˆ°Redisæµ
            try:
                stream_id = self.redis_client.xadd(
                    TWEET_STREAM_KEY,
                    tweet_data,
                    maxlen=1000,  # é™åˆ¶æµé•¿åº¦
                    approximate=True
                )
                
                # æ›´æ–°ç”¨æˆ·çŠ¶æ€
                self.state_manager.update_user_state(
                    user_id,
                    last_tweet_id=tweet_id,
                    last_success_time=time.time(),
                    last_check_time=time.time(),
                    initialized=True,
                    username=username  # ä¿å­˜ç”¨æˆ·ååˆ°çŠ¶æ€
                )
                
                logger.info(f"âœ… ç”¨æˆ· {username}(@{user_id}) æ–°æ¨æ–‡å·²æ¨é€: {tweet_id}")
                logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šæ–°æ¨æ–‡ {tweet_id}")
                return True
                
            except Exception as e:
                logger.error(f"æ¨æ–‡æ·»åŠ åˆ°Rediså¤±è´¥: {e}, æ¨æ–‡æ•°æ®: {tweet_data}")
                # å³ä½¿Rediså¤±è´¥ï¼Œä¹Ÿè¦æ›´æ–°çŠ¶æ€é¿å…é‡å¤å°è¯•
                self.state_manager.update_user_state(
                    user_id,
                    last_tweet_id=tweet_id,
                    last_check_time=time.time(),
                    username=username
                )
                logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šRediså¤±è´¥ä½†æ›´æ–°tweet_id")
                return False
                
        except ET.ParseError as e:
            logger.error(f"è§£æç”¨æˆ· {user_id} RSSå¤±è´¥: {e}")
            # è§£æå¤±è´¥ä¹Ÿè¦æ›´æ–°æ£€æŸ¥æ—¶é—´
            self.state_manager.update_user_state(
                user_id,
                last_check_time=time.time(),
                parse_error_count=user_state.get("parse_error_count", 0) + 1
            )
            logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šRSSè§£æå¤±è´¥")
            return False
        except Exception as e:
            logger.error(f"å¤„ç†RSSå†…å®¹æ—¶å‡ºé”™: {user_id}, {e}, æ¨æ–‡æ•°æ®: {tweet_data}")
            # å…¶ä»–é”™è¯¯ä¹Ÿè¦æ›´æ–°æ£€æŸ¥æ—¶é—´
            self.state_manager.update_user_state(
                user_id,
                last_check_time=time.time(),
                error_count=user_state.get("error_count", 0) + 1
            )
            logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šå¤„ç†å‡ºé”™")
            return False
    
    def parse_date(self, date_str: str) -> datetime:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return datetime.now()
            
        formats = [
            "%a, %d %b %Y %H:%M:%S %z",
            "%a, %d %b %Y %H:%M:%S",
            "%a, %d %b %Y %H:%M:%S GMT",
            "%Y-%m-%dT%H:%M:%S%z",
            "%Y-%m-%dT%H:%M:%S"
        ]
        
        # å¤„ç†GMT
        if " GMT" in date_str:
            date_str = date_str.replace(" GMT", " +0000")
        
        for fmt in formats:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue
                
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸ: {date_str}")
        return datetime.now()
    


    async def initialize_users(self):
        """åˆå§‹åŒ–ç”¨æˆ·å’ŒETagæ£€æŸ¥"""
        # åŠ è½½å…³æ³¨åˆ—è¡¨
        following_list = self.load_following_list()
        
        # åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨ä¸­çš„ç”¨æˆ·
        for user_info in following_list:
            user_id = user_info['userId']  # ä½¿ç”¨userIdä½œä¸ºç”¨æˆ·ID
            if not self.state_manager.get_user_state(user_id):
                # å¦‚æœç”¨æˆ·ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–ç”¨æˆ·çŠ¶æ€
                self.state_manager.update_user_state(
                    user_id,
                    display_name=user_info.get('name', user_info.get('username', user_id)),  # ä½¿ç”¨nameæˆ–usernameä½œä¸ºæ˜¾ç¤ºå
                    last_check=None,
                    last_tweet_id=None
                )
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(following_list)} ä¸ªå…³æ³¨ç”¨æˆ·")

        # æ‰§è¡Œåˆå§‹è´Ÿè½½å‡è¡¡
        self.perform_initial_load_balancing()
        
        # æ‰“å°è¯¦ç»†çš„å®ä¾‹ä¿¡æ¯ç”¨äºè°ƒè¯•
        self.print_instance_info()
        
        logger.info(f"åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(following_list)} ä¸ªç”¨æˆ·")
        
    def perform_initial_load_balancing(self):
        """æ‰§è¡Œåˆå§‹è´Ÿè½½å‡è¡¡"""
        # åªå¯¹å½“å‰following_listä¸­çš„ç”¨æˆ·è¿›è¡Œè´Ÿè½½å‡è¡¡ï¼Œä¸åŒ…æ‹¬å†å²ç”¨æˆ·
        following_list = self.load_following_list()
        users = [user_info['userId'] for user_info in following_list]  # åªä½¿ç”¨å½“å‰å…³æ³¨åˆ—è¡¨çš„ç”¨æˆ·
        
        total_users = len(users)
        instance_count = len(self.instances)
        
        if instance_count == 0:
            logger.error("æ²¡æœ‰å¯ç”¨çš„Nitterå®ä¾‹")
            return
            
        users_per_instance = total_users // instance_count
        extra_users = total_users % instance_count
        
        logger.info(f"å¼€å§‹åˆ†é… {total_users} ä¸ªç”¨æˆ·åˆ° {instance_count} ä¸ªå®ä¾‹")
        logger.info(f"æ¯ä¸ªå®ä¾‹å¹³å‡ {users_per_instance} ä¸ªç”¨æˆ·ï¼Œ{extra_users} ä¸ªå®ä¾‹å„å¤šåˆ†é…1ä¸ªç”¨æˆ·")
        
        # æ¸…ç©ºç°æœ‰æ˜ å°„
        self.user_instance_mapping.clear()
        for instance in self.instances:
            instance.assigned_users = 0
        
        user_index = 0
        for i, instance in enumerate(self.instances):
            # è®¡ç®—è¿™ä¸ªå®ä¾‹åº”è¯¥åˆ†é…å¤šå°‘ç”¨æˆ·
            target_users = users_per_instance + (1 if i < extra_users else 0)
            
            for _ in range(target_users):
                if user_index < len(users):
                    user_id = users[user_index]
                    self.user_instance_mapping[user_id] = instance
                    instance.assigned_users += 1
                    user_index += 1
            
            logger.info(f"å®ä¾‹ {instance.url} åˆ†é…äº† {instance.assigned_users} ä¸ªç”¨æˆ·")
        
        logger.info("åˆå§‹è´Ÿè½½å‡è¡¡å®Œæˆ")
    
    def adjust_concurrency(self, success_count: int, total_count: int, error_count: int):
        """æ ¹æ®æˆåŠŸç‡åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°"""
        if total_count == 0:
            return
            
        success_rate = success_count / total_count
        error_rate = error_count / total_count
        
        # è®°å½•æœ€è¿‘çš„é”™è¯¯ç‡
        self.recent_errors.append(error_rate)
        if len(self.recent_errors) > 5:  # åªä¿ç•™æœ€è¿‘5æ¬¡çš„è®°å½•
            self.recent_errors.pop(0)
        
        avg_error_rate = sum(self.recent_errors) / len(self.recent_errors)
        
        old_concurrent = self.current_concurrent
        
        # if avg_error_rate > 0.3:  # é”™è¯¯ç‡è¶…è¿‡30%ï¼Œå‡å°‘å¹¶å‘
        #     self.current_concurrent = max(self.min_concurrent, self.current_concurrent - 1)
        #     logger.info(f"é”™è¯¯ç‡è¿‡é«˜ ({avg_error_rate:.1%})ï¼Œé™ä½å¹¶å‘æ•°: {old_concurrent} -> {self.current_concurrent}")
        # elif avg_error_rate < 0.1 and success_rate > 0.8:  # é”™è¯¯ç‡ä½äº10%ä¸”æˆåŠŸç‡é«˜ï¼Œå¢åŠ å¹¶å‘
        #     self.current_concurrent = min(self.max_concurrent, self.current_concurrent + 1)
        #     logger.info(f"æ€§èƒ½è‰¯å¥½ (é”™è¯¯ç‡: {avg_error_rate:.1%})ï¼Œæé«˜å¹¶å‘æ•°: {old_concurrent} -> {self.current_concurrent}")
        
    async def poll_users_batch(self, user_batch: List[str]):
        """æ‰¹é‡è½®è¯¢ç”¨æˆ·"""
        batch_start = time.time()
        
        # ç»Ÿè®¡æœ¬æ‰¹æ¬¡ç”¨æˆ·çš„å®ä¾‹åˆ†å¸ƒ
        batch_instance_count = {}
        for user_id in user_batch:
            instance = self.get_instance_for_user(user_id)
            url = instance.url
            batch_instance_count[url] = batch_instance_count.get(url, 0) + 1
        
        logger.info(f"å¼€å§‹å¤„ç†æ‰¹æ¬¡: {len(user_batch)} ä¸ªç”¨æˆ·")
        logger.info(f"æ‰¹æ¬¡å®ä¾‹åˆ†å¸ƒ: {batch_instance_count}")
        logger.debug(f"ç”¨æˆ·åˆ—è¡¨: {user_batch}")
        
        async with aiohttp.ClientSession() as session:
            tasks = [self.fetch_user_rss(session, user_id) for user_id in user_batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            success_count = sum(1 for r in results if r is True)
            error_count = sum(1 for r in results if isinstance(r, Exception))
            rate_limit_count = sum(1 for r in results if isinstance(r, aiohttp.ClientResponseError) and r.status == 429)
            batch_duration = time.time() - batch_start
            
            # æ”¶é›†429é™æµçš„ç”¨æˆ·ï¼ŒåŠ å…¥å¾…å¤„ç†é˜Ÿåˆ—
            rate_limited_users = []
            batch_results = []

            for i, result in enumerate(results):
                user_id = user_batch[i]
                if isinstance(result, aiohttp.ClientResponseError) and result.status == 429:
                    rate_limited_users.append(user_id)
                    batch_results.append((user_id, False, "429 Rate Limited"))
                elif isinstance(result, Exception):
                    batch_results.append((user_id, False, str(result)))
                else:
                    batch_results.append((user_id, result, ""))

            if rate_limited_users:
                self.pending_users.extend(rate_limited_users)
                logger.info(f"ğŸ”„ {len(rate_limited_users)} ä¸ªç”¨æˆ·å› 429é™æµåŠ å…¥ä¸‹ä¸€æ‰¹æ¬¡: {rate_limited_users}")

            # æ›´æ–°æ‰¹æ¬¡ç»Ÿè®¡
            self.update_batch_stats(batch_results)

            # æ¯æ‰¹æ¬¡å®Œæˆåä¿å­˜çŠ¶æ€
            try:
                self.state_manager.save_state()
                logger.debug(f"ğŸ’¾ æ‰¹æ¬¡å®ŒæˆåçŠ¶æ€å·²ä¿å­˜")
            except Exception as e:
                logger.error(f"ğŸ’¾ æ‰¹æ¬¡çŠ¶æ€ä¿å­˜å¤±è´¥: {e}")

            logger.info(f"æ‰¹æ¬¡å®Œæˆ: {len(user_batch)} ç”¨æˆ·, {success_count} ä¸ªæœ‰æ–°æ¨æ–‡, {error_count} ä¸ªå¼‚å¸¸ (å…¶ä¸­ {rate_limit_count} ä¸ª429é™æµ), è€—æ—¶: {batch_duration:.2f}ç§’")
            
            # å¦‚æœ429é”™è¯¯å¤ªå¤šï¼Œè‡ªåŠ¨é™ä½å¹¶å‘æ•°
            if rate_limit_count > len(user_batch) * 0.5:  # è¶…è¿‡50%æ˜¯429é”™è¯¯
                logger.warning(f"429é”™è¯¯è¿‡å¤š ({rate_limit_count}/{len(user_batch)})ï¼Œå»ºè®®é™ä½å¹¶å‘æ•°")
            
            # åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
            self.adjust_concurrency(success_count, len(user_batch), error_count)
            
            # æ‰“å°ETagç»Ÿè®¡ï¼ˆæ¯10ä¸ªæ‰¹æ¬¡æ‰“å°ä¸€æ¬¡ï¼‰
            if hasattr(self, '_batch_counter'):
                self._batch_counter += 1
            else:
                self._batch_counter = 1
                
            if self._batch_counter % 10 == 0:
                self.print_cache_stats()
    
    def get_next_batch(self, users: List[str], batch_size: int, current_index: int) -> Tuple[List[str], int]:
        """è·å–ä¸‹ä¸€ä¸ªæ‰¹æ¬¡ï¼Œä¼˜å…ˆå¤„ç†å¤±è´¥çš„ç”¨æˆ· + è´Ÿè½½å‡è¡¡"""
        batch = []

        # é¦–å…ˆæ·»åŠ å¾…å¤„ç†çš„ç”¨æˆ·ï¼ˆä¸»è¦æ˜¯429é™æµç”¨æˆ·ï¼‰
        while len(batch) < batch_size and self.pending_users:
            batch.append(self.pending_users.pop(0))

        # ç„¶åä»æ­£å¸¸é˜Ÿåˆ—è¡¥å……ç”¨æˆ·
        remaining_slots = batch_size - len(batch)
        if remaining_slots > 0 and current_index < len(users):
            end_index = min(current_index + remaining_slots, len(users))
            batch.extend(users[current_index:end_index])
            current_index = end_index

        # æ–¹æ¡ˆA: æ‰“ä¹±æ‰¹æ¬¡å†…ç”¨æˆ·é¡ºåºï¼Œé¿å…å®ä¾‹é›†ä¸­
        if len(batch) > 1:
            import random
            random.shuffle(batch)

        return batch, current_index

    def get_balanced_batch(self, users: List[str], batch_size: int, current_index: int) -> Tuple[List[str], int]:
        """æ–¹æ¡ˆB: è·å–è´Ÿè½½å‡è¡¡çš„æ‰¹æ¬¡ï¼Œç¡®ä¿æ¥è‡ªä¸åŒå®ä¾‹"""
        batch = []

        # é¦–å…ˆæ·»åŠ å¾…å¤„ç†çš„ç”¨æˆ·ï¼ˆä¸»è¦æ˜¯429é™æµç”¨æˆ·ï¼‰
        while len(batch) < batch_size and self.pending_users:
            batch.append(self.pending_users.pop(0))

        remaining_slots = batch_size - len(batch)
        if remaining_slots > 0 and current_index < len(users):
            # æŒ‰å®ä¾‹åˆ†ç»„å‰©ä½™ç”¨æˆ·
            instance_users = {}
            for i in range(current_index, len(users)):
                user_id = users[i]
                if user_id in self.user_instance_mapping:
                    instance = self.user_instance_mapping[user_id]
                    instance_url = instance.url
                    if instance_url not in instance_users:
                        instance_users[instance_url] = []
                    instance_users[instance_url].append((user_id, i))

            # è½®è¯¢ä»æ¯ä¸ªå®ä¾‹é€‰æ‹©ç”¨æˆ·
            selected_users = []
            max_index = current_index

            while len(selected_users) < remaining_slots and instance_users:
                for instance_url in list(instance_users.keys()):
                    if len(selected_users) >= remaining_slots:
                        break

                    if instance_users[instance_url]:
                        user_id, user_index = instance_users[instance_url].pop(0)
                        selected_users.append(user_id)
                        max_index = max(max_index, user_index + 1)

                        # å¦‚æœè¿™ä¸ªå®ä¾‹æ²¡æœ‰æ›´å¤šç”¨æˆ·ï¼Œç§»é™¤å®ƒ
                        if not instance_users[instance_url]:
                            del instance_users[instance_url]

                # å¦‚æœæ‰€æœ‰å®ä¾‹éƒ½æ²¡æœ‰ç”¨æˆ·äº†ï¼Œé€€å‡ºå¾ªç¯
                if not instance_users:
                    break

            batch.extend(selected_users)
            current_index = max_index

        return batch, current_index

    def get_batch_instance_distribution(self, batch: List[str]) -> Dict[str, int]:
        """è·å–æ‰¹æ¬¡çš„å®ä¾‹åˆ†å¸ƒæƒ…å†µ"""
        distribution = {}
        for user_id in batch:
            if user_id in self.user_instance_mapping:
                instance = self.user_instance_mapping[user_id]
                instance_url = instance.url
                distribution[instance_url] = distribution.get(instance_url, 0) + 1
            else:
                # å¦‚æœç”¨æˆ·æ²¡æœ‰åˆ†é…å®ä¾‹ï¼Œä¸´æ—¶åˆ†é…ä¸€ä¸ª
                instance = self.get_instance_for_user(user_id)
                instance_url = instance.url
                distribution[instance_url] = distribution.get(instance_url, 0) + 1
        return distribution

    async def run(self):
        """è¿è¡Œè½®è¯¢å¼•æ“"""
        logger.info("å¯åŠ¨å¢å¼ºè½®è¯¢å¼•æ“...")
        
        # åˆå§‹åŒ–ç”¨æˆ·
        await self.initialize_users()

        # ä¸å†å‘é€æµ‹è¯•æ¨æ–‡ï¼Œé¿å…å¹²æ‰°æ—¶é—´æ’åº
        # await self.send_test_tweet()
        
        cycle_count = 0
        last_rebalance_cycle = 0
        
        while True:
            try:
                cycle_count += 1
                cycle_start = time.time()
                logger.info(f"å¼€å§‹ç¬¬ {cycle_count} è½®è½®è¯¢...")

                # æ›´æ–°è½®è¯¢å‘¨æœŸ
                self.polling_stats["current_cycle"] = cycle_count

                # é‡ç½®å½“å‰è½®è¯¢å‘¨æœŸç»Ÿè®¡
                self.reset_cycle_stats()

                # æ¯è½®æ£€æŸ¥é…ç½®æ˜¯å¦æœ‰æ›´æ–°
                if self.reload_polling_config():
                    logger.info("è½®è¯¢é…ç½®å·²æ›´æ–°")

                users = self.state_manager.get_all_users()
                if not users:
                    logger.warning("æ²¡æœ‰ç”¨æˆ·éœ€è¦è½®è¯¢")
                    await asyncio.sleep(POLL_INTERVAL)
                    continue

                # æ–¹æ¡ˆA: æ‰“ä¹±ç”¨æˆ·åˆ—è¡¨é¡ºåºï¼Œé¿å…å®ä¾‹é›†ä¸­
                import random
                random.shuffle(users)

                pending_count = len(self.pending_users)
                logger.info(f"æœ¬è½®å°†å¤„ç† {len(users)} ä¸ªç”¨æˆ·ï¼Œå¹¶å‘æ•°: {self.current_concurrent}ï¼Œå¾…å¤„ç†é˜Ÿåˆ—: {pending_count} ä¸ªç”¨æˆ·")

                # ä½¿ç”¨æ–°çš„æ‰¹æ¬¡è·å–é€»è¾‘
                batch_count = 0
                current_index = 0
                
                while current_index < len(users) or self.pending_users:
                    batch_count += 1
                    # ä½¿ç”¨è´Ÿè½½å‡è¡¡çš„æ‰¹æ¬¡è·å–æ–¹æ³•
                    batch, current_index = self.get_balanced_batch(users, self.current_concurrent, current_index)
                    
                    if not batch:  # æ²¡æœ‰æ›´å¤šç”¨æˆ·éœ€è¦å¤„ç†
                        break

                    # æ˜¾ç¤ºæ‰¹æ¬¡å®ä¾‹åˆ†å¸ƒ
                    batch_distribution = self.get_batch_instance_distribution(batch)
                    logger.info(f"å¤„ç†ç¬¬ {batch_count} æ‰¹ç”¨æˆ·...")
                    logger.info(f"æ‰¹æ¬¡å®ä¾‹åˆ†å¸ƒ: {batch_distribution}")

                    await self.poll_users_batch(batch)
                    
                    # æ‰¹æ¬¡é—´çŸ­æš‚å»¶è¿Ÿ
                    if current_index < len(users) or self.pending_users:
                        await asyncio.sleep(BATCH_DELAY)
                
                # å®Œæˆè½®è¯¢å‘¨æœŸç»Ÿè®¡
                self.finalize_cycle_stats()

                # ä¿å­˜çŠ¶æ€
                save_start = time.time()
                logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜çŠ¶æ€æ–‡ä»¶...")
                self.state_manager.save_state()
                save_duration = time.time() - save_start
                logger.info(f"ğŸ’¾ çŠ¶æ€æ–‡ä»¶ä¿å­˜å®Œæˆï¼Œè€—æ—¶: {save_duration:.2f}ç§’")

                cycle_duration = time.time() - cycle_start
                remaining_pending = len(self.pending_users)
                logger.info(f"ç¬¬ {cycle_count} è½®è½®è¯¢å®Œæˆ! æ€»è€—æ—¶: {cycle_duration:.2f}ç§’, çŠ¶æ€ä¿å­˜è€—æ—¶: {save_duration:.2f}ç§’, å‰©ä½™å¾…å¤„ç†: {remaining_pending} ä¸ªç”¨æˆ·")
                
                # æ¯10è½®æ‰“å°è´Ÿè½½åˆ†å¸ƒ
                if cycle_count % 10 == 0:
                    self.print_load_distribution()
                
                # ç­‰å¾…ä¸‹ä¸€è½®
                logger.info(f"ç­‰å¾… {POLL_INTERVAL} ç§’åå¼€å§‹ä¸‹ä¸€è½®...")
                await asyncio.sleep(POLL_INTERVAL)
                
            except Exception as e:
                logger.error(f"è½®è¯¢è¿‡ç¨‹å‡ºé”™: {e}")
                # å³ä½¿å‡ºé”™ä¹Ÿè¦ä¿å­˜çŠ¶æ€
                try:
                    logger.info(f"ğŸ’¾ å¼‚å¸¸æƒ…å†µä¸‹ä¿å­˜çŠ¶æ€...")
                    self.state_manager.save_state()
                    logger.info(f"ğŸ’¾ å¼‚å¸¸æƒ…å†µä¸‹çŠ¶æ€ä¿å­˜å®Œæˆ")
                except Exception as save_error:
                    logger.error(f"ğŸ’¾ å¼‚å¸¸æƒ…å†µä¸‹ä¿å­˜çŠ¶æ€å¤±è´¥: {save_error}")
                await asyncio.sleep(5)
    
    async def send_test_tweet(self):
        """å‘é€æµ‹è¯•æ¨æ–‡"""
        try:
            test_tweet = {
                "id": f"test_{int(time.time())}",
                "user_id": "system",
                "username": "ç³»ç»Ÿæµ‹è¯•",
                "content": f"ç³»ç»Ÿæµ‹è¯•æ¨æ–‡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                "html": "<p>ç³»ç»Ÿæµ‹è¯•æ¨æ–‡</p>",
                "published_at": datetime.now().strftime("%a, %d %b %Y %H:%M:%S"),
                "url": "#",
                "timestamp": datetime.now().isoformat(),
                "images": json.dumps([])
            }
            
            self.redis_client.xadd(TWEET_STREAM_KEY, test_tweet)
            logger.info("æµ‹è¯•æ¨æ–‡å·²å‘é€")
            
        except Exception as e:
            logger.error(f"å‘é€æµ‹è¯•æ¨æ–‡å¤±è´¥: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¢å¼ºçš„æ¨ç‰¹è½®è¯¢å¼•æ“")
    
    parser.add_argument(
        "--following-file",
        default="./config/following_list.json",
        help="å…³æ³¨åˆ—è¡¨æ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--nitter-instances",
        default="http://localhost:8080",
        help="Nitterå®ä¾‹URLï¼Œå¤šä¸ªå®ä¾‹ç”¨é€—å·åˆ†éš”"
    )
    
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¯ç”¨è°ƒè¯•æ—¥å¿—"
    )



    args = parser.parse_args()
    
    if args.debug:
        logger.setLevel(logging.DEBUG)
    
    # è§£æå®ä¾‹URL
    nitter_instances = [url.strip() for url in args.nitter_instances.split(",")]
    logger.info(f"ä½¿ç”¨Nitterå®ä¾‹: {nitter_instances}")
    
    # æ£€æŸ¥å…³æ³¨åˆ—è¡¨æ–‡ä»¶
    if not os.path.exists(args.following_file):
        logger.error(f"å…³æ³¨åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {args.following_file}")
        sys.exit(1)
    
    try:
        engine = EnhancedPollingEngine(nitter_instances, args.following_file)
        await engine.run()
    except KeyboardInterrupt:
        logger.info("æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
    except Exception as e:
        logger.error(f"å¼•æ“è¿è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main()) 