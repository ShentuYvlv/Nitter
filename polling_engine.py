import json
import time
import random
import asyncio
import aiohttp
import redis
import logging
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict, Set, Optional, Tuple
import os
import argparse
import sys
from pathlib import Path
import re
from dataclasses import dataclass
import hashlib

# åœ¨Windowsä¸Šè®¾ç½®æ­£ç¡®çš„äº‹ä»¶å¾ªç¯ç­–ç•¥
if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("enhanced_polling")

# Redisé…ç½® - ä»…ç”¨äºæ¨æ–‡æµ
REDIS_HOST = os.environ.get("REDIS_HOST", "localhost")
REDIS_PORT = int(os.environ.get("REDIS_PORT", 6379))
REDIS_DB = int(os.environ.get("REDIS_DB", 0))
REDIS_PASSWORD = os.environ.get("REDIS_PASSWORD", None)

# ç®€åŒ–çš„Redisé…ç½® - ä»…ç”¨äºæ¨æ–‡æµ
TWEET_STREAM_KEY = "tweets"

# çŠ¶æ€æ–‡ä»¶é…ç½®
STATE_FILE = "state.json"
DEFAULT_STATE = {
    "system": {
        "last_updated": None,
        "version": "1.0"
    },
    "users": {},
    "instances": {}
}

# è½®è¯¢é…ç½®
POLL_INTERVAL = 15          # ç»Ÿä¸€è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰
CONCURRENT_USERS = 10        # æ¯ä¸ªå®ä¾‹å¹¶å‘æ•° - é™ä½åˆ°3é¿å…429é”™è¯¯
REQUEST_TIMEOUT = 5         # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
MAX_RETRIES = 2             # æœ€å¤§é‡è¯•æ¬¡æ•°
RETRY_DELAYS = [0.5, 1.0]   # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
RATE_LIMIT_DELAY = 2.0      # é‡åˆ°429é”™è¯¯æ—¶çš„é¢å¤–å»¶è¿Ÿ
BATCH_DELAY = 0.5           # æ‰¹æ¬¡é—´åŸºç¡€å»¶è¿Ÿ

@dataclass
class NitterInstance:
    url: str
    weight: float = 10.0
    consecutive_failures: int = 0
    last_failure: Optional[float] = None
    active_connections: int = 0
    max_connections: int = 50
    
    # æ–°å¢ï¼šå¤šå®ä¾‹è´Ÿè½½å‡è¡¡å­—æ®µ
    assigned_users: int = 0          # åˆ†é…çš„ç”¨æˆ·æ•°
    recent_429_count: int = 0        # æœ€è¿‘çš„429é”™è¯¯æ•°
    success_rate: float = 1.0        # æˆåŠŸç‡
    avg_response_time: float = 0.0   # å¹³å‡å“åº”æ—¶é—´
    last_health_check: Optional[float] = None  # æœ€åå¥åº·æ£€æŸ¥æ—¶é—´
    is_healthy: bool = True          # å¥åº·çŠ¶æ€
    total_requests: int = 0          # æ€»è¯·æ±‚æ•°
    total_429_errors: int = 0        # æ€»429é”™è¯¯æ•°

class StateManager:
    """çŠ¶æ€ç®¡ç†å™¨ - ä½¿ç”¨JSONæ–‡ä»¶å­˜å‚¨æ‰€æœ‰çŠ¶æ€"""
    
    def __init__(self, state_file: str = STATE_FILE):
        self.state_file = Path(state_file)
        self.state = self._load_state()
        
    def _load_state(self) -> dict:
        """åŠ è½½çŠ¶æ€æ–‡ä»¶"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    state = json.load(f)
                logger.info(f"å·²åŠ è½½çŠ¶æ€æ–‡ä»¶ï¼ŒåŒ…å« {len(state.get('users', {}))} ä¸ªç”¨æˆ·")
                return state
            except (json.JSONDecodeError, IOError) as e:
                logger.error(f"è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
                
        logger.info("åˆ›å»ºæ–°çš„çŠ¶æ€æ–‡ä»¶")
        return DEFAULT_STATE.copy()
    
    def save_state(self):
        """ä¿å­˜çŠ¶æ€åˆ°æ–‡ä»¶"""
        try:
            self.state["system"]["last_updated"] = datetime.now().isoformat()
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, indent=2, ensure_ascii=False)
        except IOError as e:
            logger.error(f"ä¿å­˜çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
    
    def get_user_state(self, user_id: str) -> dict:
        """è·å–ç”¨æˆ·çŠ¶æ€"""
        return self.state["users"].get(user_id, {})
    
    def update_user_state(self, user_id: str, **kwargs):
        """æ›´æ–°ç”¨æˆ·çŠ¶æ€"""
        if user_id not in self.state["users"]:
            self.state["users"][user_id] = {}
        
        self.state["users"][user_id].update(kwargs)
        self.state["users"][user_id]["last_updated"] = datetime.now().isoformat()
    
    def get_all_users(self) -> List[str]:
        """è·å–æ‰€æœ‰ç”¨æˆ·ID"""
        return list(self.state["users"].keys())
    
    def update_instance_state(self, instance_url: str, **kwargs):
        """æ›´æ–°å®ä¾‹çŠ¶æ€"""
        if instance_url not in self.state["instances"]:
            self.state["instances"][instance_url] = {"weight": 10.0, "consecutive_failures": 0}
        
        self.state["instances"][instance_url].update(kwargs)

class EnhancedPollingEngine:
    """å¢å¼ºçš„è½®è¯¢å¼•æ“"""
    
    def __init__(self, nitter_instances: List[str], following_file: str):
        self.instances = [NitterInstance(url) for url in nitter_instances]
        self.following_file = following_file
        self.state_manager = StateManager()
        self.use_sse = True  # å¯ç”¨SSE
        self.sse_connections = {}  # å­˜å‚¨SSEè¿æ¥
        
        # å¤šå®ä¾‹è´Ÿè½½å‡è¡¡
        self.user_instance_mapping = {}  # ç”¨æˆ·åˆ°å®ä¾‹çš„æ˜ å°„
        self.instance_stats = {}         # å®ä¾‹ç»Ÿè®¡ä¿¡æ¯
        self.health_check_interval = 300 # å¥åº·æ£€æŸ¥é—´éš”ï¼ˆ5åˆ†é’Ÿï¼‰
        
        # ETagä¼˜åŒ–ç»Ÿè®¡
        self.etag_stats = {
            "total_requests": 0,
            "cache_hits": 0,  # 304å“åº”
            "cache_misses": 0,  # 200å“åº”
            "bandwidth_saved": 0,  # ä¼°ç®—èŠ‚çœçš„å¸¦å®½
            "no_etag_requests": 0  # ä¸æ”¯æŒETagçš„è¯·æ±‚
        }
        
        # åŠ¨æ€å¹¶å‘æ§åˆ¶ï¼ˆç°åœ¨æŒ‰å®ä¾‹ç®¡ç†ï¼‰
        self.current_concurrent = CONCURRENT_USERS
        self.recent_errors = []  # è®°å½•æœ€è¿‘çš„é”™è¯¯
        self.max_concurrent = 5  # æœ€å¤§å¹¶å‘æ•°
        self.min_concurrent = 1  # æœ€å°å¹¶å‘æ•°
        
        # å¤±è´¥ç”¨æˆ·é˜Ÿåˆ— - ç”¨äºé“¾å¼æ‰¹æ¬¡å¤„ç†
        self.pending_users = []  # éœ€è¦é‡æ–°å¤„ç†çš„ç”¨æˆ·ï¼ˆä¸»è¦æ˜¯429é”™è¯¯ï¼‰
        self.etag_supported = None  # ç¼“å­˜ETagæ”¯æŒçŠ¶æ€
        
        # è¿æ¥Redis - ä»…ç”¨äºæ¨æ–‡æµ
        try:
            self.redis_client = redis.Redis(
                host=REDIS_HOST, 
                port=REDIS_PORT, 
                db=REDIS_DB,
                password=REDIS_PASSWORD,
                decode_responses=True
            )
            self.redis_client.ping()
            logger.info(f"æˆåŠŸè¿æ¥åˆ°Redis: {REDIS_HOST}:{REDIS_PORT}")
                
        except redis.ConnectionError as e:
            logger.error(f"æ— æ³•è¿æ¥åˆ°Redis: {e}")
            raise
            
        # åˆå§‹åŒ–å®ä¾‹ç»Ÿè®¡
        for instance in self.instances:
            self.instance_stats[instance.url] = {
                "requests_this_cycle": 0,
                "errors_this_cycle": 0,
                "response_times": [],
                "last_reset": time.time()
            }
            
        logger.info(f"åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨ {len(self.instances)} ä¸ªNitterå®ä¾‹")
        self.print_instance_info()
        
    def print_instance_info(self):
        """æ‰“å°å®ä¾‹ä¿¡æ¯"""
        logger.info(f"=== Nitterå®ä¾‹ä¿¡æ¯ ===")
        for i, instance in enumerate(self.instances):
            logger.info(f"å®ä¾‹ {i+1}: {instance.url}")
            logger.info(f"  æƒé‡: {instance.weight}")
            logger.info(f"  å½“å‰è¿æ¥æ•°: {instance.active_connections}")
            logger.info(f"  æœ€å¤§è¿æ¥æ•°: {instance.max_connections}")
            logger.info(f"  åˆ†é…çš„ç”¨æˆ·æ•°: {instance.assigned_users}")
            logger.info(f"  æœ€è¿‘çš„429é”™è¯¯æ•°: {instance.recent_429_count}")
            logger.info(f"  æˆåŠŸç‡: {instance.success_rate:.2%}")
            logger.info(f"  å¹³å‡å“åº”æ—¶é—´: {instance.avg_response_time:.2f}ç§’")
            logger.info(f"  å¥åº·çŠ¶æ€: {'å¥åº·' if instance.is_healthy else 'ä¸å¥åº·'}")
            logger.info(f"  æ€»è¯·æ±‚æ•°: {instance.total_requests}")
            logger.info(f"  æ€»429é”™è¯¯æ•°: {instance.total_429_errors}")
        logger.info(f"======================")
        
        # æ£€æŸ¥ç”¨æˆ·åˆ†é…æ˜ å°„
        mapping_count = len(self.user_instance_mapping)
        logger.info(f"ç”¨æˆ·å®ä¾‹æ˜ å°„æ•°é‡: {mapping_count}")
        
        # ç»Ÿè®¡æ¯ä¸ªå®ä¾‹çš„åˆ†é…æƒ…å†µ
        instance_user_count = {}
        for user_id, instance in self.user_instance_mapping.items():
            url = instance.url
            instance_user_count[url] = instance_user_count.get(url, 0) + 1
        
        logger.info("å®é™…æ˜ å°„åˆ†å¸ƒ:")
        for url, count in instance_user_count.items():
            logger.info(f"  {url}: {count} ä¸ªç”¨æˆ·")
        logger.info(f"=======================")
    
    def load_following_list(self) -> List[Dict]:
        """åŠ è½½å…³æ³¨ç”¨æˆ·åˆ—è¡¨"""
        try:
            with open(self.following_file, 'r', encoding='utf-8') as f:
                following_list = json.load(f)
            logger.info(f"æˆåŠŸåŠ è½½ {len(following_list)} ä¸ªå…³æ³¨ç”¨æˆ·")
            return following_list
        except (json.JSONDecodeError, FileNotFoundError) as e:
            logger.error(f"æ— æ³•åŠ è½½å…³æ³¨åˆ—è¡¨: {e}")
            return []
            
    def get_instance_for_user(self, user_id: str) -> NitterInstance:
        """ä¸ºç‰¹å®šç”¨æˆ·é€‰æ‹©å®ä¾‹ï¼ˆæ™ºèƒ½è´Ÿè½½å‡è¡¡ï¼‰"""
        # å¦‚æœç”¨æˆ·å·²æœ‰åˆ†é…çš„å®ä¾‹ï¼Œç›´æ¥è¿”å›ï¼ˆé™¤éå®ä¾‹çœŸçš„ä¸å¥åº·ï¼‰
        if user_id in self.user_instance_mapping:
            assigned_instance = self.user_instance_mapping[user_id]
            # åªæœ‰åœ¨å®ä¾‹çœŸæ­£ä¸å¥åº·æ—¶æ‰é‡æ–°åˆ†é…ï¼ˆæ›´ä¸¥æ ¼çš„å¥åº·æ£€æŸ¥ï¼‰
            if (assigned_instance.is_healthy and 
                assigned_instance.recent_429_count < 20):  # æ”¾å®½429é”™è¯¯é™åˆ¶ï¼Œé¿å…é¢‘ç¹é‡åˆ†é…
                logger.debug(f"ç”¨æˆ· {user_id} ä½¿ç”¨å·²åˆ†é…å®ä¾‹ {assigned_instance.url}")
                return assigned_instance
            else:
                logger.info(f"ç”¨æˆ· {user_id} çš„åˆ†é…å®ä¾‹ {assigned_instance.url} ä¸å¥åº·ï¼Œé‡æ–°åˆ†é…")
                # ä»ä¸å¥åº·å®ä¾‹ç§»é™¤ç”¨æˆ·
                if assigned_instance.assigned_users > 0:
                    assigned_instance.assigned_users -= 1
        
        # ä¸ºç”¨æˆ·é‡æ–°åˆ†é…å¥åº·çš„å®ä¾‹ï¼ˆè¿™ç§æƒ…å†µåº”è¯¥å¾ˆå°‘å‘ç”Ÿï¼‰
        healthy_instances = [inst for inst in self.instances if inst.is_healthy]
        
        if not healthy_instances:
            logger.warning("æ²¡æœ‰å¥åº·çš„å®ä¾‹å¯ç”¨ï¼Œä½¿ç”¨æƒé‡æœ€é«˜çš„å®ä¾‹")
            selected_instance = max(self.instances, key=lambda x: x.weight)
        else:
            # é€‰æ‹©è´Ÿè½½æœ€è½»çš„å¥åº·å®ä¾‹ï¼ˆä½¿ç”¨å¤šä¸ªæŒ‡æ ‡ï¼‰
            selected_instance = min(healthy_instances, 
                                  key=lambda x: (x.assigned_users, x.recent_429_count, -x.success_rate))
        
        # æ›´æ–°ç”¨æˆ·åˆ†é…
        self.user_instance_mapping[user_id] = selected_instance
        selected_instance.assigned_users += 1
        
        logger.info(f"ç”¨æˆ· {user_id} é‡æ–°åˆ†é…åˆ°å®ä¾‹ {selected_instance.url} (åˆ†é…ç”¨æˆ·æ•°: {selected_instance.assigned_users})")
        return selected_instance
    
    def update_instance_stats(self, instance: NitterInstance, success: bool, response_time: float, is_429: bool = False):
        """æ›´æ–°å®ä¾‹ç»Ÿè®¡ä¿¡æ¯"""
        current_time = time.time()
        stats = self.instance_stats[instance.url]
        
        # æ›´æ–°åŸºç¡€ç»Ÿè®¡
        instance.total_requests += 1
        stats["requests_this_cycle"] += 1
        stats["response_times"].append(response_time)
        
        if is_429:
            instance.total_429_errors += 1
            instance.recent_429_count += 1
            stats["errors_this_cycle"] += 1
            logger.warning(f"å®ä¾‹ {instance.url} é‡åˆ°429é”™è¯¯ï¼Œå½“å‰å‘¨æœŸ429é”™è¯¯æ•°: {instance.recent_429_count}")
        elif not success:
            stats["errors_this_cycle"] += 1
        
        # è®¡ç®—æˆåŠŸç‡
        if instance.total_requests > 0:
            instance.success_rate = (instance.total_requests - instance.total_429_errors) / instance.total_requests
        
        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        if stats["response_times"]:
            instance.avg_response_time = sum(stats["response_times"]) / len(stats["response_times"])
        
        # æ›´æ–°å¥åº·çŠ¶æ€
        self.update_instance_health(instance)
        
        # å®šæœŸé‡ç½®å‘¨æœŸç»Ÿè®¡
        if current_time - stats["last_reset"] > 300:  # 5åˆ†é’Ÿé‡ç½®ä¸€æ¬¡
            instance.recent_429_count = 0
            stats["requests_this_cycle"] = 0
            stats["errors_this_cycle"] = 0
            stats["response_times"] = []
            stats["last_reset"] = current_time
            logger.info(f"é‡ç½®å®ä¾‹ {instance.url} å‘¨æœŸç»Ÿè®¡")
    
    def update_instance_health(self, instance: NitterInstance):
        """æ›´æ–°å®ä¾‹å¥åº·çŠ¶æ€"""
        # å¥åº·åˆ¤æ–­æ ‡å‡†
        max_429_errors = 15  # å‘¨æœŸå†…æœ€å¤§429é”™è¯¯æ•°
        min_success_rate = 0.6  # æœ€å°æˆåŠŸç‡
        
        was_healthy = instance.is_healthy
        
        # åˆ¤æ–­æ˜¯å¦å¥åº·
        instance.is_healthy = (
            instance.recent_429_count < max_429_errors and
            instance.success_rate >= min_success_rate
        )
        
        # å¦‚æœå¥åº·çŠ¶æ€å‘ç”Ÿå˜åŒ–ï¼Œè®°å½•æ—¥å¿—
        if was_healthy != instance.is_healthy:
            status = "å¥åº·" if instance.is_healthy else "ä¸å¥åº·"
            logger.warning(f"å®ä¾‹ {instance.url} çŠ¶æ€å˜æ›´ä¸º: {status}")
            logger.info(f"  - æœ€è¿‘429é”™è¯¯: {instance.recent_429_count}")
            logger.info(f"  - æˆåŠŸç‡: {instance.success_rate:.2%}")
            logger.info(f"  - åˆ†é…ç”¨æˆ·: {instance.assigned_users}")
    
    def rebalance_users(self):
        """é‡æ–°å¹³è¡¡ç”¨æˆ·åˆ†é…"""
        healthy_instances = [inst for inst in self.instances if inst.is_healthy]
        
        if len(healthy_instances) == 0:
            logger.error("æ²¡æœ‰å¥åº·çš„å®ä¾‹ï¼Œæ— æ³•é‡æ–°å¹³è¡¡")
            return
        
        total_users = len(self.user_instance_mapping)
        users_per_instance = total_users // len(healthy_instances)
        extra_users = total_users % len(healthy_instances)
        
        logger.info(f"å¼€å§‹é‡æ–°å¹³è¡¡ {total_users} ä¸ªç”¨æˆ·åˆ° {len(healthy_instances)} ä¸ªå¥åº·å®ä¾‹")
        
        # é‡ç½®æ‰€æœ‰å®ä¾‹çš„ç”¨æˆ·è®¡æ•°
        for instance in self.instances:
            instance.assigned_users = 0
        
        # é‡æ–°åˆ†é…ç”¨æˆ·
        user_list = list(self.user_instance_mapping.keys())
        user_index = 0
        
        for i, instance in enumerate(healthy_instances):
            # è®¡ç®—è¿™ä¸ªå®ä¾‹åº”è¯¥åˆ†é…å¤šå°‘ç”¨æˆ·
            target_users = users_per_instance + (1 if i < extra_users else 0)
            
            for _ in range(target_users):
                if user_index < len(user_list):
                    user_id = user_list[user_index]
                    self.user_instance_mapping[user_id] = instance
                    instance.assigned_users += 1
                    user_index += 1
        
        logger.info("ç”¨æˆ·é‡æ–°å¹³è¡¡å®Œæˆ")
        self.print_load_distribution()
    
    def print_load_distribution(self):
        """æ‰“å°è´Ÿè½½åˆ†å¸ƒæƒ…å†µ"""
        logger.info("=== å®ä¾‹è´Ÿè½½åˆ†å¸ƒ ===")
        for instance in self.instances:
            status = "ğŸŸ¢" if instance.is_healthy else "ğŸ”´"
            logger.info(f"{status} {instance.url}:")
            logger.info(f"  åˆ†é…ç”¨æˆ·: {instance.assigned_users}")
            logger.info(f"  429é”™è¯¯: {instance.recent_429_count}")
            logger.info(f"  æˆåŠŸç‡: {instance.success_rate:.1%}")
            logger.info(f"  å“åº”æ—¶é—´: {instance.avg_response_time:.2f}s")
        logger.info("==================")
    
    async def setup_sse_connection(self, user_id: str):
        """ä¸ºç”¨æˆ·å»ºç«‹SSEè¿æ¥"""
        instance = self.get_instance_for_user(user_id)
        sse_url = f"{instance.url}/stream/user/{user_id}"
        
        try:
            session = aiohttp.ClientSession()
            response = await session.get(
                sse_url,
                headers={"Accept": "text/event-stream"},
                timeout=aiohttp.ClientTimeout(total=None)  # SSEéœ€è¦æ— é™è¶…æ—¶
            )
            
            if response.status == 200:
                self.sse_connections[user_id] = {
                    "session": session,
                    "response": response,
                    "instance": instance
                }
                instance.active_connections += 1
                logger.info(f"ä¸ºç”¨æˆ· {user_id} å»ºç«‹SSEè¿æ¥: {sse_url}")
                
                # å¯åŠ¨SSEæ•°æ®å¤„ç†
                asyncio.create_task(self.process_sse_stream(user_id))
                return True
            else:
                await session.close()
                logger.warning(f"SSEè¿æ¥å¤±è´¥: {user_id}, çŠ¶æ€ç : {response.status}")
                return False
                
        except Exception as e:
            logger.error(f"å»ºç«‹SSEè¿æ¥æ—¶å‡ºé”™: {user_id}, {e}")
            return False
    
    async def process_sse_stream(self, user_id: str):
        """å¤„ç†SSEæ•°æ®æµ"""
        connection_info = self.sse_connections.get(user_id)
        if not connection_info:
            return
            
        response = connection_info["response"]
        session = connection_info["session"]
        
        try:
            async for line in response.content:
                line = line.decode('utf-8').strip()
                
                if line.startswith('data: '):
                    data = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€
                    try:
                        tweet_data = json.loads(data)
                        # å¤„ç†æ¨æ–‡æ•°æ®...
                        await self.process_tweet_from_sse(user_id, tweet_data)
                    except json.JSONDecodeError:
                        continue
                        
        except Exception as e:
            logger.error(f"å¤„ç†SSEæµæ—¶å‡ºé”™: {user_id}, {e}")
        finally:
            # æ¸…ç†è¿æ¥
            if user_id in self.sse_connections:
                del self.sse_connections[user_id]
                connection_info["instance"].active_connections -= 1
            await session.close()
    
    async def fetch_with_etag_optimization(self, session: aiohttp.ClientSession, 
                                          user_id: str) -> bool:
        """å¸¦ETagä¼˜åŒ–çš„è·å–æ–¹æ³•ï¼ˆå¤šå®ä¾‹ç‰ˆæœ¬ï¼‰"""
        instance = self.get_instance_for_user(user_id)
        url = f"{instance.url}/{user_id}/rss"
        
        # è·å–ä¿å­˜çš„ETag
        user_state = self.state_manager.get_user_state(user_id)
        headers = {}
        etag_used = False
        
        if user_state.get("etag") and self.etag_supported:
            headers["If-None-Match"] = user_state["etag"]
            etag_used = True
            self.etag_stats["total_requests"] += 1
            logger.debug(f"ç”¨æˆ· {user_id} ä½¿ç”¨ETag: {user_state['etag'][:20]}...")
        elif not self.etag_supported:
            # è®°å½•ä¸æ”¯æŒETagçš„è¯·æ±‚
            self.etag_stats["no_etag_requests"] += 1
        
        start_time = time.time()
        success = False
        is_429 = False
        
        try:
            async with session.get(url, headers=headers, timeout=5) as response:
                request_duration = time.time() - start_time
                
                if response.status == 304:
                    # ç¼“å­˜å‘½ä¸­ï¼
                    if etag_used:
                        self.etag_stats["cache_hits"] += 1
                        # ä¼°ç®—èŠ‚çœçš„å¸¦å®½ï¼ˆå¹³å‡RSSå¤§å°çº¦50KBï¼‰
                        self.etag_stats["bandwidth_saved"] += 50 * 1024
                    
                    logger.info(f"ğŸ¯ ç”¨æˆ· {user_id} ETagç¼“å­˜å‘½ä¸­ï¼è€—æ—¶: {request_duration:.2f}ç§’ [å®ä¾‹: {instance.url}]")
                    
                    # ç¼“å­˜å‘½ä¸­ä¹Ÿè¦æ›´æ–°æ£€æŸ¥æ—¶é—´
                    self.state_manager.update_user_state(
                        user_id,
                        last_check_time=time.time(),
                        last_success_time=time.time()
                    )
                    logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šETagç¼“å­˜å‘½ä¸­")
                    
                    success = True
                    result = False
                    
                elif response.status == 200:
                    if etag_used:
                        self.etag_stats["cache_misses"] += 1
                    
                    # ä¿å­˜æ–°çš„ETag
                    if "ETag" in response.headers and self.etag_supported:
                        new_etag = response.headers["ETag"]
                        self.state_manager.update_user_state(
                            user_id, 
                            etag=new_etag,
                            last_check_time=time.time()
                        )
                        logger.debug(f"ç”¨æˆ· {user_id} ä¿å­˜æ–°ETag: {new_etag[:20]}...")
                    
                    content = await response.text()
                    logger.info(f"ğŸ“¥ ç”¨æˆ· {user_id} è·å–æ–°å†…å®¹ï¼Œè€—æ—¶: {request_duration:.2f}ç§’ï¼Œå¤§å°: {len(content)} å­—èŠ‚ [å®ä¾‹: {instance.url}]")
                    
                    result = await self.process_rss_content(user_id, content)
                    success = True
                    
                elif response.status == 429:
                    logger.warning(f"â° ç”¨æˆ· {user_id} é‡åˆ°é€Ÿç‡é™åˆ¶: HTTP 429 [å®ä¾‹: {instance.url}]")
                    is_429 = True
                    # 429é”™è¯¯ä¹Ÿè¦æ›´æ–°æ£€æŸ¥æ—¶é—´
                    self.state_manager.update_user_state(
                        user_id,
                        last_check_time=time.time(),
                        rate_limit_count=user_state.get("rate_limit_count", 0) + 1
                    )
                    logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼š429é™æµé”™è¯¯")
                    
                    # æŠ›å‡ºç‰¹æ®Šå¼‚å¸¸ä»¥ä¾¿åœ¨æ‰¹æ¬¡å¤„ç†ä¸­è¯†åˆ«
                    raise aiohttp.ClientResponseError(
                        request_info=response.request_info,
                        history=response.history,
                        status=429,
                        message="Rate Limited"
                    )
                else:
                    logger.warning(f"âŒ ç”¨æˆ· {user_id} è·å–å¤±è´¥: HTTP {response.status} [å®ä¾‹: {instance.url}]")
                    # å…¶ä»–HTTPé”™è¯¯ä¹Ÿè¦æ›´æ–°æ£€æŸ¥æ—¶é—´
                    self.state_manager.update_user_state(
                        user_id,
                        last_check_time=time.time(),
                        http_error_count=user_state.get("http_error_count", 0) + 1
                    )
                    logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šHTTP {response.status} é”™è¯¯")
                    success = False
                    result = False
                    
        except aiohttp.ClientResponseError as e:
            request_duration = time.time() - start_time
            if e.status == 429:
                is_429 = True
                logger.warning(f"ğŸ’¥ ç”¨æˆ· {user_id} 429é™æµé”™è¯¯ï¼Œè€—æ—¶: {request_duration:.2f}ç§’ [å®ä¾‹: {instance.url}]")
            else:
                logger.error(f"ğŸ’¥ ç”¨æˆ· {user_id} HTTPé”™è¯¯: {e.status}ï¼Œè€—æ—¶: {request_duration:.2f}ç§’ [å®ä¾‹: {instance.url}]")
            success = False
            result = False
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿ä¸Šå±‚å¤„ç†
            
        except Exception as e:
            request_duration = time.time() - start_time
            logger.error(f"ğŸ’¥ ç”¨æˆ· {user_id} ç½‘ç»œé”™è¯¯: {e}ï¼Œè€—æ—¶: {request_duration:.2f}ç§’ [å®ä¾‹: {instance.url}]")
            # ç½‘ç»œé”™è¯¯ä¹Ÿè¦æ›´æ–°æ£€æŸ¥æ—¶é—´
            self.state_manager.update_user_state(
                user_id,
                last_check_time=time.time(),
                network_error_count=user_state.get("network_error_count", 0) + 1
            )
            logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šç½‘ç»œé”™è¯¯")
            success = False
            result = False
            
        finally:
            # æ›´æ–°å®ä¾‹ç»Ÿè®¡
            request_duration = time.time() - start_time
            self.update_instance_stats(instance, success, request_duration, is_429)
            
        return result if success else False
    
    def print_etag_stats(self):
        """æ‰“å°ETagç»Ÿè®¡ä¿¡æ¯"""
        total_all_requests = (self.etag_stats["total_requests"] + 
                             self.etag_stats["no_etag_requests"])
        
        if total_all_requests > 0:
            if self.etag_stats["total_requests"] > 0:
                hit_rate = (self.etag_stats["cache_hits"] / 
                           self.etag_stats["total_requests"]) * 100
                saved_mb = self.etag_stats["bandwidth_saved"] / (1024 * 1024)
                
                logger.info(f"""
ğŸ“Š ETagä¼˜åŒ–ç»Ÿè®¡:
   æ”¯æŒETagè¯·æ±‚: {self.etag_stats["total_requests"]}
   ç¼“å­˜å‘½ä¸­: {self.etag_stats["cache_hits"]} ({hit_rate:.1f}%)
   ç¼“å­˜å¤±è¯¯: {self.etag_stats["cache_misses"]}
   ä¸æ”¯æŒETagè¯·æ±‚: {self.etag_stats["no_etag_requests"]}
   èŠ‚çœå¸¦å®½: {saved_mb:.1f} MB
                """)
            else:
                logger.info(f"""
ğŸ“Š ETagä¼˜åŒ–ç»Ÿè®¡:
   æ€»è¯·æ±‚æ•°: {total_all_requests}
   ä¸æ”¯æŒETagè¯·æ±‚: {self.etag_stats["no_etag_requests"]} (100%)
   ETagåŠŸèƒ½æœªå¯ç”¨ - Nitterå®ä¾‹ä¸æ”¯æŒETag
                """)
        else:
            logger.info("ğŸ“Š ETagä¼˜åŒ–ç»Ÿè®¡: æš‚æ— æ•°æ®")
    
    async def process_rss_content(self, user_id: str, content: str) -> bool:
        """å¤„ç†RSSå†…å®¹"""
        tweet_data = None  # é¢„å®šä¹‰å˜é‡é¿å…ä½œç”¨åŸŸé—®é¢˜
        
        try:
            root = ET.fromstring(content)
            items = root.findall(".//item")
            
            if not items:
                logger.debug(f"ç”¨æˆ· {user_id} RSSæ²¡æœ‰æ¨æ–‡é¡¹ç›®")
                return False
                
            # å°è¯•ä»RSSä¸­æå–ç”¨æˆ·å
            username = user_id  # é»˜è®¤ä½¿ç”¨user_id
            
            # æ–¹æ³•1: ä»channel titleä¸­æå–ç”¨æˆ·å (æ ¼å¼é€šå¸¸æ˜¯ "/ Twitter")
            channel_title = root.find(".//channel/title")
            if channel_title is not None and channel_title.text:
                title_text = channel_title.text
                # å¤„ç†ä¸åŒçš„æ ‡é¢˜æ ¼å¼
                if "/ Twitter" in title_text:
                    username = title_text.replace("/ Twitter", "").strip()
                elif "/ Nitter" in title_text:
                    username = title_text.replace("/ Nitter", "").strip()
                elif " / @" in title_text:
                    # å¤„ç† 'Aster / @Aster_DEX' è¿™ç§æ ¼å¼ï¼Œåªå–"/"å‰é¢çš„éƒ¨åˆ†
                    username = title_text.split(" / @")[0].strip()
                elif " /" in title_text:
                    # å¤„ç†å…¶ä»–åŒ…å«"/"çš„æ ¼å¼ï¼Œå–"/"å‰é¢çš„éƒ¨åˆ†
                    username = title_text.split(" /")[0].strip()
                else:
                    # å¦‚æœæ²¡æœ‰ç‰¹æ®Šæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨æ ‡é¢˜
                    username = title_text.strip()
                
                logger.debug(f"ä»RSS channel titleæå–ç”¨æˆ·å: {user_id} -> {username} (åŸæ ‡é¢˜: {title_text})")
                
            # æ–¹æ³•2: ä»ç¬¬ä¸€ä¸ªæ¨æ–‡çš„ä½œè€…ä¿¡æ¯ä¸­æå– (å¤‡é€‰æ–¹æ¡ˆ)
            if username == user_id:
                latest_item = items[0]
                # å°è¯•ä»creatoræˆ–authorå­—æ®µè·å–
                creator = latest_item.find(".//{http://purl.org/dc/elements/1.1/}creator")
                if creator is not None and creator.text:
                    username = creator.text.strip()
                    logger.debug(f"ä»RSS creatorå­—æ®µæå–ç”¨æˆ·å: {user_id} -> {username}")
                
                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•ä»æ¨æ–‡æ ‡é¢˜ä¸­æå– (é€šå¸¸æ ¼å¼æ˜¯ "RT by username: content")
                if username == user_id:
                    title = latest_item.find("title")
                    if title is not None and title.text:
                        title_text = title.text
                        if "RT by " in title_text and ":" in title_text:
                            # æå– "RT by username:" ä¸­çš„ç”¨æˆ·å
                            parts = title_text.split("RT by ")[1].split(":")[0]
                            username = parts.strip()
                            logger.debug(f"ä»æ¨æ–‡æ ‡é¢˜æå–ç”¨æˆ·å: {user_id} -> {username}")
                
            # è·å–æœ€æ–°æ¨æ–‡
            latest_item = items[0]
            link = latest_item.find("link")
            title = latest_item.find("title")
            description = latest_item.find("description")
            pub_date = latest_item.find("pubDate")
            
            # æå–å›¾ç‰‡URL
            images = []
            
            # æ–¹æ³•1: ä»enclosureå…ƒç´ ä¸­æå–å›¾ç‰‡
            enclosures = latest_item.findall("enclosure")
            for enclosure in enclosures:
                if enclosure.get("type", "").startswith("image/"):
                    images.append(enclosure.get("url", ""))
            
            # æ–¹æ³•2: ä»descriptionçš„HTMLå†…å®¹ä¸­æå–å›¾ç‰‡
            if description is not None and description.text:
                # åŒ¹é…imgæ ‡ç­¾ä¸­çš„srcå±æ€§
                img_pattern = r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>'
                img_matches = re.findall(img_pattern, description.text, re.IGNORECASE)
                images.extend(img_matches)
                
                # åŒ¹é…å…¶ä»–å¯èƒ½çš„å›¾ç‰‡URLæ¨¡å¼
                url_pattern = r'https?://[^\s<>"]+\.(?:jpg|jpeg|png|gif|webp)(?:\?[^\s<>"]*)?'
                url_matches = re.findall(url_pattern, description.text, re.IGNORECASE)
                images.extend(url_matches)
            
            # å»é‡å¹¶è¿‡æ»¤æœ‰æ•ˆçš„å›¾ç‰‡URLï¼ŒåŒæ—¶ä¿®å¤ç«¯å£å·é—®é¢˜
            unique_images = []
            for img_url in images:
                if img_url and img_url not in unique_images:
                    # è¿‡æ»¤æ‰ä¸€äº›æ— æ•ˆçš„URL
                    if not img_url.startswith('data:') and len(img_url) > 10:
                        # ä¿®å¤ç«¯å£å·é—®é¢˜ï¼šå¦‚æœURLæ˜¯localhostä½†æ²¡æœ‰ç«¯å£ï¼Œæ·»åŠ 8080ç«¯å£
                        if img_url.startswith('http://localhost/') and ':8080' not in img_url:
                            img_url = img_url.replace('http://localhost/', 'http://localhost:8080/')
                            logger.debug(f"ä¿®å¤å›¾ç‰‡URLç«¯å£: {img_url}")
                        unique_images.append(img_url)
            
            if unique_images:
                logger.debug(f"ç”¨æˆ· {user_id} æ¨æ–‡åŒ…å« {len(unique_images)} å¼ å›¾ç‰‡: {unique_images[:2]}...")  # åªæ˜¾ç¤ºå‰2ä¸ªURL
            
            # ä¿®å¤æ£€æŸ¥é€»è¾‘ - æ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ–‡æœ¬å†…å®¹
            if link is None or not link.text:
                logger.warning(f"ç”¨æˆ· {user_id} æ¨æ–‡é“¾æ¥ä¸ºç©º")
                return False
            
            if title is None or not title.text:
                logger.warning(f"ç”¨æˆ· {user_id} æ¨æ–‡æ ‡é¢˜ä¸ºç©º")
                return False
                
            if description is None or not description.text:
                logger.warning(f"ç”¨æˆ· {user_id} æ¨æ–‡æè¿°ä¸ºç©º")
                return False
                
            if pub_date is None or not pub_date.text:
                logger.warning(f"ç”¨æˆ· {user_id} æ¨æ–‡å‘å¸ƒæ—¶é—´ä¸ºç©º")
                return False
            
            tweet_id = link.text.split("/")[-1]
            user_state = self.state_manager.get_user_state(user_id)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°æ¨æ–‡
            current_last_tweet_id = user_state.get("last_tweet_id")
            logger.debug(f"ç”¨æˆ· {user_id} å½“å‰ä¿å­˜çš„tweet_id: {current_last_tweet_id}, æ–°tweet_id: {tweet_id}")
            
            if current_last_tweet_id == tweet_id:
                logger.debug(f"ç”¨æˆ· {user_id} æ¨æ–‡æœªæ›´æ–°ï¼Œè·³è¿‡")
                # å³ä½¿æ²¡æœ‰æ–°æ¨æ–‡ï¼Œä¹Ÿè¦æ›´æ–°æœ€åæ£€æŸ¥æ—¶é—´å’ŒæˆåŠŸæ—¶é—´
                self.state_manager.update_user_state(
                    user_id,
                    last_check_time=time.time(),
                    last_success_time=time.time(),
                    username=username
                )
                logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šæ— æ–°æ¨æ–‡ä½†æ›´æ–°æ£€æŸ¥æ—¶é—´")
                return False
                        
            # è§£æå‘å¸ƒæ—¶é—´
            try:
                pub_time = self.parse_date(pub_date.text)
                today = datetime.now().date()
                
                # å¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œï¼Œåªæ¨é€å½“æ—¥æ¨æ–‡
                if not user_state.get("initialized") and pub_time.date() != today:
                    logger.info(f"ç”¨æˆ· {user_id} é¦–æ¬¡è¿è¡Œï¼Œè·³è¿‡éå½“æ—¥æ¨æ–‡: {pub_time.date()}")
                    self.state_manager.update_user_state(
                        user_id, 
                        last_tweet_id=tweet_id, 
                        initialized=True,
                        last_check_time=time.time(),
                        username=username
                    )
                    logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šé¦–æ¬¡è¿è¡Œåˆå§‹åŒ–")
                    return False
                    
            except Exception as e:
                logger.warning(f"è§£æç”¨æˆ· {user_id} æ¨æ–‡æ—¶é—´å¤±è´¥: {e}")
                pub_time = datetime.now()
            
            # æ„å»ºæ¨æ–‡æ•°æ®
            tweet_data = {
                "id": tweet_id,
                "user_id": user_id,
                "username": username,
                "content": title.text or "",
                "html": description.text or "",
                "published_at": pub_date.text,
                "url": link.text,
                "timestamp": pub_time.isoformat(),
                "images": json.dumps(unique_images)
            }
                        
            # æ·»åŠ åˆ°Redisæµ
            try:
                stream_id = self.redis_client.xadd(
                    TWEET_STREAM_KEY,
                    tweet_data,
                    maxlen=1000,  # é™åˆ¶æµé•¿åº¦
                    approximate=True
                )
                
                # æ›´æ–°ç”¨æˆ·çŠ¶æ€
                self.state_manager.update_user_state(
                    user_id,
                    last_tweet_id=tweet_id,
                    last_success_time=time.time(),
                    last_check_time=time.time(),
                    initialized=True,
                    username=username  # ä¿å­˜ç”¨æˆ·ååˆ°çŠ¶æ€
                )
                
                logger.info(f"âœ… ç”¨æˆ· {username}(@{user_id}) æ–°æ¨æ–‡å·²æ¨é€: {tweet_id}")
                logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šæ–°æ¨æ–‡ {tweet_id}")
                return True
                
            except Exception as e:
                logger.error(f"æ¨æ–‡æ·»åŠ åˆ°Rediså¤±è´¥: {e}, æ¨æ–‡æ•°æ®: {tweet_data}")
                # å³ä½¿Rediså¤±è´¥ï¼Œä¹Ÿè¦æ›´æ–°çŠ¶æ€é¿å…é‡å¤å°è¯•
                self.state_manager.update_user_state(
                    user_id,
                    last_tweet_id=tweet_id,
                    last_check_time=time.time(),
                    username=username
                )
                logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šRediså¤±è´¥ä½†æ›´æ–°tweet_id")
                return False
                
        except ET.ParseError as e:
            logger.error(f"è§£æç”¨æˆ· {user_id} RSSå¤±è´¥: {e}")
            # è§£æå¤±è´¥ä¹Ÿè¦æ›´æ–°æ£€æŸ¥æ—¶é—´
            self.state_manager.update_user_state(
                user_id,
                last_check_time=time.time(),
                parse_error_count=user_state.get("parse_error_count", 0) + 1
            )
            logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šRSSè§£æå¤±è´¥")
            return False
        except Exception as e:
            logger.error(f"å¤„ç†RSSå†…å®¹æ—¶å‡ºé”™: {user_id}, {e}, æ¨æ–‡æ•°æ®: {tweet_data}")
            # å…¶ä»–é”™è¯¯ä¹Ÿè¦æ›´æ–°æ£€æŸ¥æ—¶é—´
            self.state_manager.update_user_state(
                user_id,
                last_check_time=time.time(),
                error_count=user_state.get("error_count", 0) + 1
            )
            logger.debug(f"ğŸ”„ æ›´æ–°ç”¨æˆ· {user_id} çŠ¶æ€ï¼šå¤„ç†å‡ºé”™")
            return False
    
    def parse_date(self, date_str: str) -> datetime:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return datetime.now()
            
        formats = [
            "%a, %d %b %Y %H:%M:%S %z",
            "%a, %d %b %Y %H:%M:%S",
            "%a, %d %b %Y %H:%M:%S GMT",
            "%Y-%m-%dT%H:%M:%S%z",
            "%Y-%m-%dT%H:%M:%S"
        ]
        
        # å¤„ç†GMT
        if " GMT" in date_str:
            date_str = date_str.replace(" GMT", " +0000")
        
        for fmt in formats:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue
                
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸ: {date_str}")
        return datetime.now()
    
    async def check_etag_support(self):
        """æ£€æŸ¥Nitterå®ä¾‹æ˜¯å¦æ”¯æŒETag"""
        test_users = self.state_manager.get_all_users()[:3]  # å–å‰3ä¸ªç”¨æˆ·æµ‹è¯•
        
        if not test_users:
            logger.info("æ²¡æœ‰ç”¨æˆ·å¯ç”¨äºETagæ”¯æŒæ£€æŸ¥")
            self.etag_supported = False
            return
            
        logger.info("ğŸ” æ£€æŸ¥Nitterå®ä¾‹ETagæ”¯æŒ...")
        etag_found = False
        
        async with aiohttp.ClientSession() as session:
            for user_id in test_users:
                instance = self.get_instance_for_user(user_id)
                url = f"{instance.url}/{user_id}/rss"
                
                try:
                    async with session.get(url, timeout=5) as response:
                        if response.status == 200:
                            if "ETag" in response.headers:
                                logger.info(f"âœ… å®ä¾‹ {instance.url} æ”¯æŒETag: {response.headers['ETag'][:20]}...")
                                etag_found = True
                                break
                            else:
                                logger.warning(f"âš ï¸ å®ä¾‹ {instance.url} ä¸æ”¯æŒETag (ç”¨æˆ·: {user_id})")
                        else:
                            logger.debug(f"æµ‹è¯•ç”¨æˆ· {user_id} è¿”å›çŠ¶æ€: {response.status}")
                            
                except Exception as e:
                    logger.debug(f"ETagæ”¯æŒæ£€æŸ¥å¤±è´¥ {user_id}: {e}")
                    continue
        
        self.etag_supported = etag_found
        if etag_found:
            logger.info("âœ… ETagç¼“å­˜ä¼˜åŒ–å·²å¯ç”¨")
        else:
            logger.info("âŒ ETagç¼“å­˜ä¼˜åŒ–å·²ç¦ç”¨ - å®ä¾‹ä¸æ”¯æŒ")
        logger.info("ETagæ”¯æŒæ£€æŸ¥å®Œæˆ")

    async def initialize_users(self):
        """åˆå§‹åŒ–ç”¨æˆ·å’ŒETagæ£€æŸ¥"""
        # åŠ è½½å…³æ³¨åˆ—è¡¨
        following_list = self.load_following_list()
        
        # åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨ä¸­çš„ç”¨æˆ·
        for user_info in following_list:
            user_id = user_info['userId']  # ä½¿ç”¨userIdä½œä¸ºç”¨æˆ·ID
            if not self.state_manager.get_user_state(user_id):
                # å¦‚æœç”¨æˆ·ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–ç”¨æˆ·çŠ¶æ€
                self.state_manager.update_user_state(
                    user_id,
                    display_name=user_info.get('name', user_info.get('username', user_id)),  # ä½¿ç”¨nameæˆ–usernameä½œä¸ºæ˜¾ç¤ºå
                    last_check=None,
                    last_tweet_id=None,
                    etag=None
                )
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(following_list)} ä¸ªå…³æ³¨ç”¨æˆ·")
        
        # æ£€æŸ¥ETagæ”¯æŒ
        await self.check_etag_support()
        
        # æ‰§è¡Œåˆå§‹è´Ÿè½½å‡è¡¡
        self.perform_initial_load_balancing()
        
        # æ‰“å°è¯¦ç»†çš„å®ä¾‹ä¿¡æ¯ç”¨äºè°ƒè¯•
        self.print_instance_info()
        
        logger.info(f"åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(following_list)} ä¸ªç”¨æˆ·")
        
    def perform_initial_load_balancing(self):
        """æ‰§è¡Œåˆå§‹è´Ÿè½½å‡è¡¡"""
        # åªå¯¹å½“å‰following_listä¸­çš„ç”¨æˆ·è¿›è¡Œè´Ÿè½½å‡è¡¡ï¼Œä¸åŒ…æ‹¬å†å²ç”¨æˆ·
        following_list = self.load_following_list()
        users = [user_info['userId'] for user_info in following_list]  # åªä½¿ç”¨å½“å‰å…³æ³¨åˆ—è¡¨çš„ç”¨æˆ·
        
        total_users = len(users)
        instance_count = len(self.instances)
        
        if instance_count == 0:
            logger.error("æ²¡æœ‰å¯ç”¨çš„Nitterå®ä¾‹")
            return
            
        users_per_instance = total_users // instance_count
        extra_users = total_users % instance_count
        
        logger.info(f"å¼€å§‹åˆ†é… {total_users} ä¸ªç”¨æˆ·åˆ° {instance_count} ä¸ªå®ä¾‹")
        logger.info(f"æ¯ä¸ªå®ä¾‹å¹³å‡ {users_per_instance} ä¸ªç”¨æˆ·ï¼Œ{extra_users} ä¸ªå®ä¾‹å„å¤šåˆ†é…1ä¸ªç”¨æˆ·")
        
        # æ¸…ç©ºç°æœ‰æ˜ å°„
        self.user_instance_mapping.clear()
        for instance in self.instances:
            instance.assigned_users = 0
        
        user_index = 0
        for i, instance in enumerate(self.instances):
            # è®¡ç®—è¿™ä¸ªå®ä¾‹åº”è¯¥åˆ†é…å¤šå°‘ç”¨æˆ·
            target_users = users_per_instance + (1 if i < extra_users else 0)
            
            for _ in range(target_users):
                if user_index < len(users):
                    user_id = users[user_index]
                    self.user_instance_mapping[user_id] = instance
                    instance.assigned_users += 1
                    user_index += 1
            
            logger.info(f"å®ä¾‹ {instance.url} åˆ†é…äº† {instance.assigned_users} ä¸ªç”¨æˆ·")
        
        logger.info("åˆå§‹è´Ÿè½½å‡è¡¡å®Œæˆ")
    
    def adjust_concurrency(self, success_count: int, total_count: int, error_count: int):
        """æ ¹æ®æˆåŠŸç‡åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°"""
        if total_count == 0:
            return
            
        success_rate = success_count / total_count
        error_rate = error_count / total_count
        
        # è®°å½•æœ€è¿‘çš„é”™è¯¯ç‡
        self.recent_errors.append(error_rate)
        if len(self.recent_errors) > 5:  # åªä¿ç•™æœ€è¿‘5æ¬¡çš„è®°å½•
            self.recent_errors.pop(0)
        
        avg_error_rate = sum(self.recent_errors) / len(self.recent_errors)
        
        old_concurrent = self.current_concurrent
        
        # if avg_error_rate > 0.3:  # é”™è¯¯ç‡è¶…è¿‡30%ï¼Œå‡å°‘å¹¶å‘
        #     self.current_concurrent = max(self.min_concurrent, self.current_concurrent - 1)
        #     logger.info(f"é”™è¯¯ç‡è¿‡é«˜ ({avg_error_rate:.1%})ï¼Œé™ä½å¹¶å‘æ•°: {old_concurrent} -> {self.current_concurrent}")
        # elif avg_error_rate < 0.1 and success_rate > 0.8:  # é”™è¯¯ç‡ä½äº10%ä¸”æˆåŠŸç‡é«˜ï¼Œå¢åŠ å¹¶å‘
        #     self.current_concurrent = min(self.max_concurrent, self.current_concurrent + 1)
        #     logger.info(f"æ€§èƒ½è‰¯å¥½ (é”™è¯¯ç‡: {avg_error_rate:.1%})ï¼Œæé«˜å¹¶å‘æ•°: {old_concurrent} -> {self.current_concurrent}")
        
    async def poll_users_batch(self, user_batch: List[str]):
        """æ‰¹é‡è½®è¯¢ç”¨æˆ·"""
        batch_start = time.time()
        
        # ç»Ÿè®¡æœ¬æ‰¹æ¬¡ç”¨æˆ·çš„å®ä¾‹åˆ†å¸ƒ
        batch_instance_count = {}
        for user_id in user_batch:
            instance = self.get_instance_for_user(user_id)
            url = instance.url
            batch_instance_count[url] = batch_instance_count.get(url, 0) + 1
        
        logger.info(f"å¼€å§‹å¤„ç†æ‰¹æ¬¡: {len(user_batch)} ä¸ªç”¨æˆ·")
        logger.info(f"æ‰¹æ¬¡å®ä¾‹åˆ†å¸ƒ: {batch_instance_count}")
        logger.debug(f"ç”¨æˆ·åˆ—è¡¨: {user_batch}")
        
        async with aiohttp.ClientSession() as session:
            tasks = [self.fetch_with_etag_optimization(session, user_id) for user_id in user_batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            success_count = sum(1 for r in results if r is True)
            error_count = sum(1 for r in results if isinstance(r, Exception))
            rate_limit_count = sum(1 for r in results if isinstance(r, aiohttp.ClientResponseError) and r.status == 429)
            batch_duration = time.time() - batch_start
            
            # æ”¶é›†429é™æµçš„ç”¨æˆ·ï¼ŒåŠ å…¥å¾…å¤„ç†é˜Ÿåˆ—
            rate_limited_users = []
            for i, result in enumerate(results):
                if isinstance(result, aiohttp.ClientResponseError) and result.status == 429:
                    rate_limited_users.append(user_batch[i])
            
            if rate_limited_users:
                self.pending_users.extend(rate_limited_users)
                logger.info(f"ğŸ”„ {len(rate_limited_users)} ä¸ªç”¨æˆ·å› 429é™æµåŠ å…¥ä¸‹ä¸€æ‰¹æ¬¡: {rate_limited_users}")
            
            logger.info(f"æ‰¹æ¬¡å®Œæˆ: {len(user_batch)} ç”¨æˆ·, {success_count} ä¸ªæœ‰æ–°æ¨æ–‡, {error_count} ä¸ªå¼‚å¸¸ (å…¶ä¸­ {rate_limit_count} ä¸ª429é™æµ), è€—æ—¶: {batch_duration:.2f}ç§’")
            
            # å¦‚æœ429é”™è¯¯å¤ªå¤šï¼Œè‡ªåŠ¨é™ä½å¹¶å‘æ•°
            if rate_limit_count > len(user_batch) * 0.5:  # è¶…è¿‡50%æ˜¯429é”™è¯¯
                logger.warning(f"429é”™è¯¯è¿‡å¤š ({rate_limit_count}/{len(user_batch)})ï¼Œå»ºè®®é™ä½å¹¶å‘æ•°")
            
            # åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
            self.adjust_concurrency(success_count, len(user_batch), error_count)
            
            # æ‰“å°ETagç»Ÿè®¡ï¼ˆæ¯10ä¸ªæ‰¹æ¬¡æ‰“å°ä¸€æ¬¡ï¼‰
            if hasattr(self, '_batch_counter'):
                self._batch_counter += 1
            else:
                self._batch_counter = 1
                
            if self._batch_counter % 10 == 0:
                self.print_etag_stats()
    
    def get_next_batch(self, users: List[str], batch_size: int, current_index: int) -> Tuple[List[str], int]:
        """è·å–ä¸‹ä¸€ä¸ªæ‰¹æ¬¡ï¼Œä¼˜å…ˆå¤„ç†å¤±è´¥çš„ç”¨æˆ·"""
        batch = []
        
        # é¦–å…ˆæ·»åŠ å¾…å¤„ç†çš„ç”¨æˆ·ï¼ˆä¸»è¦æ˜¯429é™æµç”¨æˆ·ï¼‰
        while len(batch) < batch_size and self.pending_users:
            batch.append(self.pending_users.pop(0))
        
        # ç„¶åä»æ­£å¸¸é˜Ÿåˆ—è¡¥å……ç”¨æˆ·
        remaining_slots = batch_size - len(batch)
        if remaining_slots > 0 and current_index < len(users):
            end_index = min(current_index + remaining_slots, len(users))
            batch.extend(users[current_index:end_index])
            current_index = end_index
        
        return batch, current_index

    async def run(self):
        """è¿è¡Œè½®è¯¢å¼•æ“"""
        logger.info("å¯åŠ¨å¢å¼ºè½®è¯¢å¼•æ“...")
        
        # åˆå§‹åŒ–ç”¨æˆ·
        await self.initialize_users()
        
        # å‘é€æµ‹è¯•æ¨æ–‡
        await self.send_test_tweet()
        
        cycle_count = 0
        last_rebalance_cycle = 0
        
        while True:
            try:
                cycle_count += 1
                cycle_start = time.time()
                logger.info(f"å¼€å§‹ç¬¬ {cycle_count} è½®è½®è¯¢...")
                
                users = self.state_manager.get_all_users()
                if not users:
                    logger.warning("æ²¡æœ‰ç”¨æˆ·éœ€è¦è½®è¯¢")
                    await asyncio.sleep(POLL_INTERVAL)
                    continue
                
                pending_count = len(self.pending_users)
                logger.info(f"æœ¬è½®å°†å¤„ç† {len(users)} ä¸ªç”¨æˆ·ï¼Œå¹¶å‘æ•°: {self.current_concurrent}ï¼Œå¾…å¤„ç†é˜Ÿåˆ—: {pending_count} ä¸ªç”¨æˆ·")
                
                # æ¯5è½®æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°å¹³è¡¡
                if cycle_count - last_rebalance_cycle >= 5:
                    unhealthy_instances = [inst for inst in self.instances if not inst.is_healthy]
                    if unhealthy_instances:
                        logger.warning(f"å‘ç° {len(unhealthy_instances)} ä¸ªä¸å¥åº·å®ä¾‹ï¼Œæ‰§è¡Œé‡æ–°å¹³è¡¡")
                        self.rebalance_users()
                        last_rebalance_cycle = cycle_count
                    else:
                        logger.info("æ‰€æœ‰å®ä¾‹å¥åº·ï¼Œè·³è¿‡é‡æ–°å¹³è¡¡")
                
                # ä½¿ç”¨æ–°çš„æ‰¹æ¬¡è·å–é€»è¾‘
                batch_count = 0
                current_index = 0
                
                while current_index < len(users) or self.pending_users:
                    batch_count += 1
                    batch, current_index = self.get_next_batch(users, self.current_concurrent, current_index)
                    
                    if not batch:  # æ²¡æœ‰æ›´å¤šç”¨æˆ·éœ€è¦å¤„ç†
                        break
                        
                    logger.info(f"å¤„ç†ç¬¬ {batch_count} æ‰¹ç”¨æˆ·...")
                    await self.poll_users_batch(batch)
                    
                    # æ‰¹æ¬¡é—´çŸ­æš‚å»¶è¿Ÿ
                    if current_index < len(users) or self.pending_users:
                        await asyncio.sleep(BATCH_DELAY)
                
                # ä¿å­˜çŠ¶æ€
                save_start = time.time()
                logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜çŠ¶æ€æ–‡ä»¶...")
                self.state_manager.save_state()
                save_duration = time.time() - save_start
                logger.info(f"ğŸ’¾ çŠ¶æ€æ–‡ä»¶ä¿å­˜å®Œæˆï¼Œè€—æ—¶: {save_duration:.2f}ç§’")
                
                cycle_duration = time.time() - cycle_start
                remaining_pending = len(self.pending_users)
                logger.info(f"ç¬¬ {cycle_count} è½®è½®è¯¢å®Œæˆ! æ€»è€—æ—¶: {cycle_duration:.2f}ç§’, çŠ¶æ€ä¿å­˜è€—æ—¶: {save_duration:.2f}ç§’, å‰©ä½™å¾…å¤„ç†: {remaining_pending} ä¸ªç”¨æˆ·")
                
                # æ¯10è½®æ‰“å°è´Ÿè½½åˆ†å¸ƒ
                if cycle_count % 10 == 0:
                    self.print_load_distribution()
                
                # ç­‰å¾…ä¸‹ä¸€è½®
                logger.info(f"ç­‰å¾… {POLL_INTERVAL} ç§’åå¼€å§‹ä¸‹ä¸€è½®...")
                await asyncio.sleep(POLL_INTERVAL)
                
            except Exception as e:
                logger.error(f"è½®è¯¢è¿‡ç¨‹å‡ºé”™: {e}")
                # å³ä½¿å‡ºé”™ä¹Ÿè¦ä¿å­˜çŠ¶æ€
                try:
                    logger.info(f"ğŸ’¾ å¼‚å¸¸æƒ…å†µä¸‹ä¿å­˜çŠ¶æ€...")
                    self.state_manager.save_state()
                    logger.info(f"ğŸ’¾ å¼‚å¸¸æƒ…å†µä¸‹çŠ¶æ€ä¿å­˜å®Œæˆ")
                except Exception as save_error:
                    logger.error(f"ğŸ’¾ å¼‚å¸¸æƒ…å†µä¸‹ä¿å­˜çŠ¶æ€å¤±è´¥: {save_error}")
                await asyncio.sleep(5)
    
    async def send_test_tweet(self):
        """å‘é€æµ‹è¯•æ¨æ–‡"""
        try:
            test_tweet = {
                "id": f"test_{int(time.time())}",
                "user_id": "system",
                "username": "ç³»ç»Ÿæµ‹è¯•",
                "content": f"ç³»ç»Ÿæµ‹è¯•æ¨æ–‡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                "html": "<p>ç³»ç»Ÿæµ‹è¯•æ¨æ–‡</p>",
                "published_at": datetime.now().strftime("%a, %d %b %Y %H:%M:%S"),
                "url": "#",
                "timestamp": datetime.now().isoformat(),
                "images": json.dumps([])
            }
            
            self.redis_client.xadd(TWEET_STREAM_KEY, test_tweet)
            logger.info("æµ‹è¯•æ¨æ–‡å·²å‘é€")
            
        except Exception as e:
            logger.error(f"å‘é€æµ‹è¯•æ¨æ–‡å¤±è´¥: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¢å¼ºçš„æ¨ç‰¹è½®è¯¢å¼•æ“")
    
    parser.add_argument(
        "--following-file",
        default="./config/following_list.json",
        help="å…³æ³¨åˆ—è¡¨æ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--nitter-instances",
        default="http://localhost:8080",
        help="Nitterå®ä¾‹URLï¼Œå¤šä¸ªå®ä¾‹ç”¨é€—å·åˆ†éš”"
    )
    
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¯ç”¨è°ƒè¯•æ—¥å¿—"
    )
    
    args = parser.parse_args()
    
    if args.debug:
        logger.setLevel(logging.DEBUG)
    
    # è§£æå®ä¾‹URL
    nitter_instances = [url.strip() for url in args.nitter_instances.split(",")]
    logger.info(f"ä½¿ç”¨Nitterå®ä¾‹: {nitter_instances}")
    
    # æ£€æŸ¥å…³æ³¨åˆ—è¡¨æ–‡ä»¶
    if not os.path.exists(args.following_file):
        logger.error(f"å…³æ³¨åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {args.following_file}")
        sys.exit(1)
    
    try:
        engine = EnhancedPollingEngine(nitter_instances, args.following_file)
        await engine.run()
    except KeyboardInterrupt:
        logger.info("æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
    except Exception as e:
        logger.error(f"å¼•æ“è¿è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main()) 